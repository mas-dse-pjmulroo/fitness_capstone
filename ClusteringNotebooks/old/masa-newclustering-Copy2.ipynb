{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "sc = SparkContext.getOrCreate()\n",
    "from pyspark.sql.functions import desc\n",
    "#traces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "#for ordered dict for traces implementation\n",
    "from collections import OrderedDict\n",
    "#from pyspark.sql.types import VectorUDT\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "#import sources.endomondolib as endo\n",
    "#import sources.pysparkconvenience as ps\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.sql.types import DoubleType\n",
    "#for timedelta manipulation\n",
    "from math import fabs\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#kmeans\n",
    "#from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from  pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SQLContext\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql.functions import col\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.mpl_style = 'default'\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc =SparkContext()\n",
    "\n",
    "\n",
    "pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\")\n",
    "if not \"pyspark-shell\" in pyspark_submit_args: pyspark_submit_args += \" pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "#%load_ext autotime\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#del min\n",
    "#del max\n",
    "\n",
    "data_users='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_with_hr_users.csv'\n",
    "\n",
    "sqlContext = HiveContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.62 ms\n"
     ]
    }
   ],
   "source": [
    "def df_from_csv(csv_file):\n",
    "    text = sc.textFile(csv_file)\\\n",
    "        .map(lambda line: line.split(\",\"))\n",
    "    #didn't work with take(1). believe returns \n",
    "    #different object then first()\n",
    "    schema = text.first()\n",
    "    data = text.filter(lambda x: x != schema)\n",
    "    df = sqlContext.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "def fix_df(df):\n",
    "    return df.rdd.toDF(df.schema.names)\n",
    "\n",
    "\n",
    "def change_column_names(df, old_names, new_names):\n",
    "    pass\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 835 µs\n"
     ]
    }
   ],
   "source": [
    "#df_from_csv('f_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 72.5 ms\n"
     ]
    }
   ],
   "source": [
    "#minMaxScaler wrapper since originalMin/Max is only implemented in 2.0\n",
    "class scaler_wrapper():\n",
    "    mmModel = ''\n",
    "    originalMin = ''\n",
    "    originalMax = ''   \n",
    "    uf = udf(lambda x: 'new_value', StringType())\n",
    "    s_udf = udf(lambda value: value*self.mmModel.getWithStd() + self.mmModel.getWithMean())\n",
    "    \n",
    "    \n",
    "    def __init__(self, inputCol, outputCol, s_min = 0, s_max = 0):\n",
    "        #self.mmModel = MinMaxScaler(inputCol=inputCol, outputCol=outputCol)\n",
    "        self.mmModel = StandardScaler(inputCol=inputCol, outputCol=outputCol)\n",
    "        self.mmModel.setWithMean(True)\n",
    "        #self.mmModel.setMin(s_min)\n",
    "        #self.mmModel.setMax(s_max)\n",
    "        self.in_column = inputCol\n",
    "        \n",
    "    def get_input_col_name(self):\n",
    "        return self.mmModel.getInputCol()\n",
    "\n",
    "    def getMax(self):\n",
    "        return self.mmModel.getMax()\n",
    "    \n",
    "    def getMin(self):\n",
    "        return self.mmModel.getMin()\n",
    "    \n",
    "    def describe(self):\n",
    "        print 'describe'\n",
    "    \n",
    "    def fit(self, df):\n",
    "        col = self.mmModel.getInputCol()\n",
    "        #self.originalMin = df.select(col).rdd.flatMap(lambda x: x[0]).min()\n",
    "        #self.originalMax = df.select(col).rdd.flatMap(lambda x: x[0]).max()\n",
    "        return self.mmModel.fit(df)\n",
    "    \n",
    "    #denormalize the value\n",
    "    def denormalize(self, value):\n",
    "        #udf = UserDefinedFunction(lambda x: 'new_value', StringType())\n",
    "        print 'deno'\n",
    "        return 5\n",
    "        #return value*self.mmModel.getWithStd() + self.mmModel.getWithMean()\n",
    "        #return 0\n",
    "        #v = (value-self.getMin())*\\\n",
    "        #    (self.originalMax - self.originalMin)*\\\n",
    "        #    (self.getMax()-self.getMin()) + self.originalMin\n",
    "        #if v or v == 0:\n",
    "        #    return v\n",
    "        #else:\n",
    "        #    return -999\n",
    "        \n",
    "    def denormalize_df(self, df):\n",
    "        col = self.mmModel.getInputCol()\n",
    "        print col\n",
    "        \n",
    "        \n",
    "    def normalize(self, value):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fe639407210>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.1 ms\n"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 972 ms\n"
     ]
    }
   ],
   "source": [
    "#calculate sse\n",
    "def compute_sse(a,b):\n",
    "    total = 0\n",
    "    if len(a) != len(b):\n",
    "        print 'bad input'\n",
    "        return 99999\n",
    "    else:\n",
    "        for i in range(len(a)):\n",
    "            total += math.pow(float(a[i])-float(b[i]),2)\n",
    "    return math.sqrt(total)\n",
    "\n",
    "def change_type_col_double(df, col):\n",
    "    df = df.withColumn(col, df[col].cast(\"double\"))\n",
    "    return df.na.fill(0)\n",
    "\n",
    "#change column type\n",
    "def change_type_cols_double(df, list_cols):\n",
    "    for col_name in list_cols:\n",
    "        df = df.withColumn(col_name, df[col_name].cast(\"double\"))\n",
    "    return df.na.fill(0)\n",
    "\n",
    "def normalize_vectorize_df(df, list_cols, scale_dict={}):\n",
    "    n_df = vectorize_columns(df, list_cols)\n",
    "    n_df, scalerModels = normalize_df(n_df, list_cols, scale_dict)\n",
    "    return n_df, scalerModels\n",
    "#vectorize the column\n",
    "#keeps the original name\n",
    "def vectorize_columns(df, list_cols, debug=False):\n",
    "    a = datetime.now()\n",
    "    tmp_col_name = 'temp'\n",
    "    for col_name in list_cols:\n",
    "        vectorize = udf(lambda vs: Vectors.dense(vs), VectorUDT())\n",
    "        df = df.withColumn(tmp_col_name, vectorize(df[col_name])).drop(col_name)\\\n",
    "            .withColumnRenamed(tmp_col_name, col_name)\n",
    "    b = datetime.now()\n",
    "    print 'vectorize done in:', b-a\n",
    "    return df\n",
    "\n",
    "##scaler wrapper usage\n",
    "#generate normalized dataframe\n",
    "#keep the original column names\n",
    "#returns:\n",
    "#   (df, {col_name:scalerModel, col_name:scalerModel,....})\n",
    "def normalize_df_old(df, list_cols, scale_dict = {}, debug=False):\n",
    "    tmp_col_name = 'temp'\n",
    "    r_dict = {}\n",
    "    index = 0\n",
    "    for col_name in list_cols:\n",
    "        if debug:\n",
    "            print '------- normalize ', col_name,'-------'\n",
    "        a = datetime.now()\n",
    "        scale_value = 1\n",
    "        if col_name in scale_dict:\n",
    "            scale_value = scale_dict[col_name]\n",
    "            print scale_value\n",
    "        #scaler is the wrapper instance\n",
    "        scaler = scaler_wrapper(inputCol=col_name, outputCol=tmp_col_name, s_max = scale_value)\n",
    "        scalerModel = scaler.fit(df)\n",
    "        b = datetime.now()\n",
    "        if debug:\n",
    "            print b-a\n",
    "        df = scalerModel.transform(df).drop(col_name)\\\n",
    "            .withColumnRenamed(tmp_col_name, col_name)\n",
    "        #r_dict[col_name] = scaler\n",
    "        r_dict[index] = scaler\n",
    "        c =datetime.now()\n",
    "        if debug:\n",
    "            print c-b\n",
    "        index+=1\n",
    "    return df, r_dict\n",
    "\n",
    "\n",
    "#   (df, {col_name:scalerModel, col_name:scalerModel,....})\n",
    "def normalize_df(df, list_cols, scale_dict = {}, debug=False):\n",
    "    tmp_col_name = 'temp'\n",
    "    #print 'masa1', df.select(['geo_distance','diff_altitude','workoutid']).describe().show(10,False)\n",
    "\n",
    "    r_dict = {}\n",
    "    index = 0\n",
    "    for col_name in list_cols:\n",
    "        if debug:\n",
    "            print '------- normalize ', col_name,'-------'\n",
    "        a = datetime.now()\n",
    "        scale_value = 1\n",
    "        if col_name in scale_dict:\n",
    "            scale_value = scale_dict[col_name]\n",
    "            print scale_value\n",
    "        #scaler is the wrapper instance\n",
    "        scaler = scaler_wrapper(inputCol=col_name, outputCol=tmp_col_name, s_max = scale_value)\n",
    "        scalerModel = scaler.fit(df)\n",
    "        b = datetime.now()\n",
    "        if debug:\n",
    "            print b-a\n",
    "        df = scalerModel.transform(df).drop(col_name)\\\n",
    "            .withColumnRenamed(tmp_col_name, col_name)\n",
    "        #take out values based on std\n",
    "        #print df.show()\n",
    "        df = df.withColumn(col_name, unvector(df[col_name]))\n",
    "        df = df.filter(df[col_name]<3)\n",
    "        #print df.count() , 'a'\n",
    "        #df = df.filter(df[col_name]>0.1)\n",
    "        #print df.count(), 'b'\n",
    "        #print 'pd'\n",
    "        # f_cluster_df.select(route_cols).filter(f_cluster_df['diff_altitude']<3)\\\n",
    "    #%%!.filter(f_cluster_df['geo_distance']<3).toPandas().describe()\n",
    "        #unv_df = df.select(['diff_altitude', 'geo_distance'])\n",
    "        #unv_df = unv_df.withColumn('diff_altitude', unvector(df['diff_altitude']))\n",
    "        #unv_df = unv_df.withColumn('geo_distance', unvector(df['geo_distance']))\n",
    "        \n",
    "        \n",
    "        #print unv_df.select(list_cols).toPandas().describe()\n",
    "        #r_dict[col_name] = scaler\n",
    "        #r_dict[index] = scaler\n",
    "        r_dict[col_name] = scalerModel\n",
    "        c =datetime.now()\n",
    "        if debug:\n",
    "            print c-b\n",
    "        index+=1\n",
    "    return df, r_dict\n",
    "\n",
    "\n",
    "# MinMaxScaler doesn't have originalMin(only supported in 2.0). made one above cell with wrapper class\n",
    "# #generate normalized dataframe\n",
    "# #keep the original column names\n",
    "# #returns:\n",
    "# #   (df, {col_name:scalerModel, col_name:scalerModel,....})\n",
    "# def normalize_df(df, list_cols):\n",
    "#     tmp_col_name = 'temp'\n",
    "#     r_dict = {}\n",
    "#     for col_name in list_cols:\n",
    "#         scaler = MinMaxScaler(inputCol=col_name, outputCol=tmp_col_name)\n",
    "#         scalerModel = scaler.fit(df)\n",
    "#         df = scalerModel.transform(df).drop(col_name)\\\n",
    "#             .withColumnRenamed(tmp_col_name, col_name)\n",
    "#         r_dict[col_name] = [scaler, scalerModel]\n",
    "#     return df, r_dict\n",
    "\n",
    "#input: (MLLIB KMEANS)\n",
    "#   cols_to_cluster: list of column names to cluster\n",
    "# def cluster_summary_df(df, cols_to_cluster, num_clusters = 2):\n",
    "#     vecAssembler = VectorAssembler(inputCols=cols_to_cluster,\\\n",
    "#         outputCol='features')\n",
    "#     df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "#     df_to_cluster = df_to_cluster.select('features')\n",
    "#     df_to_cluster = df_to_cluster.rdd\\\n",
    "#         .map(lambda row : Vectors.dense([item for item in row]))\n",
    "#     clusters = KMeans.train(df_to_cluster, num_clusters,\\\n",
    "#                            maxIterations = 10,\\\n",
    "#                         initializationMode=\"random\")\n",
    "    \n",
    "#     return clusters\n",
    "\n",
    "\n",
    "# #create feature column from list of cols\n",
    "# def create_feature_column(df, cols_to_cluster):\n",
    "#     #make dup columns of the ones that will be vectorized\n",
    "#     normalized_names = [i+'_n' for i in cols_to_cluster]\n",
    "    \n",
    "#     norm_and_woutid = [i+'_n' for i in cols_to_cluster]\n",
    "#     norm_and_woutid.append('workoutid')\n",
    "\n",
    "#     normalized_df = df\n",
    "    \n",
    "#     for c in range(len(cols_to_cluster)):\n",
    "#         normalized_df = normalized_df.withColumnRenamed(\\\n",
    "#                         cols_to_cluster[c], normalized_names[c])\n",
    "#     normalized_df = normalized_df.select(norm_and_woutid)\n",
    "\n",
    "    \n",
    "#     df = df.join(normalized_df, (df['workoutid']==normalized_df['workoutid']),\\\n",
    "#                 'inner').drop(normalized_df['workoutid'])\n",
    "#     #df = df.crossJoin(normalized_df)\n",
    "#     vecAssembler = VectorAssembler(inputCols=normalized_names,\\\n",
    "#         outputCol='features')\n",
    "\n",
    "#     df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "\n",
    "#     #normed_df = df_to_cluster.select(normalized_names)\n",
    "#     return df_to_cluster   \n",
    "    \n",
    "def create_feature_column(df, cols_to_cluster):\n",
    "    vecAssembler = VectorAssembler(inputCols=cols_to_cluster,\\\n",
    "        outputCol='features')\n",
    "\n",
    "    df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "    \n",
    "    #normed_df = df_to_cluster.select(normalized_names)\n",
    "    return df_to_cluster   \n",
    "    \n",
    "    \n",
    "    \n",
    "#ML KMEANS\n",
    "#returns KMeanModel, df_to_cluster\n",
    "def cluster_summary_df(df, cols_to_cluster, num_clusters = 2, init_var=True, bisectingKMean = True,\\\n",
    "                      debug = False):\n",
    "    a = datetime.now()\n",
    "\n",
    "#     #make dup columns of the ones that will be vectorized\n",
    "#     normalized_names = [i+'_n' for i in cols_to_cluster]\n",
    "    \n",
    "#     norm_and_woutid = [i+'_n' for i in cols_to_cluster]\n",
    "#     norm_and_woutid.append('workoutid')\n",
    "#     #make copy\n",
    "#     a = datetime.now()\n",
    "#     normalized_df = df\n",
    "    \n",
    "#     b = datetime.now()\n",
    "#     print b-a , 'df copy'\n",
    "    \n",
    "#     for c in range(len(cols_to_cluster)):\n",
    "#         normalized_df = normalized_df.withColumnRenamed(\\\n",
    "#                         cols_to_cluster[c], normalized_names[c])\n",
    "#     normalized_df = normalized_df.select(norm_and_woutid)\n",
    "\n",
    "    \n",
    "#     df = df.join(normalized_df, (df['workoutid']==normalized_df['workoutid']),\\\n",
    "#                 'inner').drop(normalized_df['workoutid'])\n",
    "#     #df = df.crossJoin(normalized_df)\n",
    "#     vecAssembler = VectorAssembler(inputCols=normalized_names,\\\n",
    "#         outputCol='features')\n",
    "\n",
    "#     df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "\n",
    "#     normed_df = df_to_cluster.select(normalized_names)\n",
    "    \n",
    "    \n",
    "#    df_to_cluster = df_to_cluster.select(['workoutid','features'])\n",
    "    #df_to_cluster = df_to_cluster.rdd\\\n",
    "     #   .map(lambda row : Vectors.dense([item for item in row]))\n",
    "    #print df.count(), 'masa3'\n",
    "    df_to_cluster = create_feature_column(df, cols_to_cluster)\n",
    "    #print df_to_cluster.count(), 'masa4'\n",
    "        \n",
    "        \n",
    "    if not bisectingKMean:\n",
    "        if not init_var:\n",
    "            kmeans = KMeans(k=num_clusters, seed=1, initMode=\"random\",\\\n",
    "                           featuresCol=cols_to_cluster)\n",
    "        else:\n",
    "            kmeans = KMeans(k=num_clusters, seed=1, initMode=\"k-means||\")\n",
    "    else:\n",
    "        kmeans = KMeans(k=num_clusters, seed=1, initMode=\"k-means||\")\n",
    "        #kmeans = BisectingKMeans().setK(num_clusters).setSeed(1).setMinDivisibleClusterSize(10)\n",
    "\n",
    "    #print df_to_cluster.show()\n",
    "    model = kmeans.fit(df_to_cluster)\n",
    "    \n",
    "    b = datetime.now()\n",
    "    if debug:\n",
    "        print 'clustering in ', b-a\n",
    "    return model, df_to_cluster\n",
    "\n",
    "\n",
    "\n",
    "# def cluster_summary_df(df, cols_to_cluster, num_clusters = 2, init_var=False):\n",
    "#     #make dup columns of the ones that will be vectorized\n",
    "#     normalized_names = [i+'_n' for i in cols_to_cluster]\n",
    "    \n",
    "#     norm_and_woutid = [i+'_n' for i in cols_to_cluster]\n",
    "#     norm_and_woutid.append('workoutid')\n",
    "#     #make copy\n",
    "#     a = datetime.now()\n",
    "#     normalized_df = df.copy()\n",
    "    \n",
    "#     b = datetime.now()\n",
    "#     print b-a , 'df copy'\n",
    "    \n",
    "#     for c in range(len(cols_to_cluster)):\n",
    "#         normalized_df = normalized_df.withColumnRenamed(\\\n",
    "#                         cols_to_cluster[c], normalized_names[c])\n",
    "#     normalized_df = normalized_df.select(norm_and_woutid)\n",
    "\n",
    "    \n",
    "#     df = df.join(normalized_df, (df['workoutid']==normalized_df['workoutid']),\\\n",
    "#                 'inner').drop(normalized_df['workoutid'])\n",
    "#     #df = df.crossJoin(normalized_df)\n",
    "#     vecAssembler = VectorAssembler(inputCols=normalized_names,\\\n",
    "#         outputCol='features')\n",
    "\n",
    "#     df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "\n",
    "#     normed_df = df_to_cluster.select(normalized_names)\n",
    "    \n",
    "    \n",
    "# #    df_to_cluster = df_to_cluster.select(['workoutid','features'])\n",
    "#     #df_to_cluster = df_to_cluster.rdd\\\n",
    "#      #   .map(lambda row : Vectors.dense([item for item in row]))\n",
    "        \n",
    "#     print 'clustering times'\n",
    "#     a = datetime.now()\n",
    "#     if not init_var:\n",
    "#         kmeans = KMeans(k=num_clusters, seed=1, initMode=\"random\",\\\n",
    "#                        featuresCol=cols_to_cluster)\n",
    "#     else:\n",
    "#         kmeans = KMeans(k=num_clusters, seed=1, initMode=\"k-means||\")\n",
    "#     b = datetime.now()\n",
    "#     print b - a\n",
    "    \n",
    "#     model = kmeans.fit(df_to_cluster)\n",
    "    \n",
    "#     c = datetime.now()\n",
    "#     print c - b\n",
    "#     return model, df_to_cluster\n",
    "\n",
    "    \n",
    "#returns clusters. show cluster centerse by clusters.clusterCenters.\n",
    "#output:\n",
    "#   [0]: KMeanModel\n",
    "#   [1]: DF that will be denormalized\n",
    "#   [2]: ScalerModels\n",
    "#   [3]: error\n",
    "#   [4]: DF with features\n",
    "def cluster_df(df, cols_to_normalize, num_clusters = 2, return_sse = False, cols_to_cluster = [], init_var=True,\\\n",
    "              scale_dict = {}, bisectingKMean = True, debug = False):\n",
    "    print 'cluster start'\n",
    "    if not cols_to_cluster:\n",
    "        cols_to_cluster = cols_to_normalize\n",
    "    df = change_type_cols_double(df, cols_to_normalize)\n",
    "    #print 'masa2', df.select(['geo_distance','diff_altitude','workoutid']).describe().show(10,False)\n",
    "    df = vectorize_columns(df, cols_to_normalize, debug)\n",
    "    #print 'masa3', df.select(['geo_distance','diff_altitude','workoutid']).describe().show(10,False)\n",
    "    df, scalerModels = normalize_df(df, cols_to_cluster, scale_dict, debug)\n",
    "    #print 'masa4', df.select(['geo_distance','diff_altitude','workoutid']).describe().show(10,False)\n",
    "    \n",
    "    ##*******************\n",
    "    #df is object to cluster. if creating kmean models off of subset of cluster, \n",
    "    #make subset here\n",
    "    \n",
    "    ##*******************\n",
    "    \n",
    "    r_obj = cluster_summary_df(df, cols_to_cluster, num_clusters, init_var, bisectingKMean, \\\n",
    "                              debug)\n",
    "    sse = 'none'\n",
    "    if return_sse:\n",
    "        #calculate SSE for this kmean model\n",
    "        kmodel = r_obj[0]\n",
    "        df_to_cluster = r_obj[1]\n",
    "        centers = kmodel.clusterCenters()\n",
    "        prediction_df = kmodel.transform(df_to_cluster)\n",
    "\n",
    "        #set up rdd\n",
    "        rdd = prediction_df.rdd.map(lambda (a,b):\\\n",
    "                                   (a, centers[b]))\n",
    "        sse = rdd.map(lambda (a,b): compute_sse(a,b)).sum()\n",
    "    return r_obj[0], df, scalerModels, sse, r_obj[1]\n",
    "    #return r_obj[0], r_obj[1], scalerModels, sse\n",
    "\n",
    "def denormalize_centers(list_centers, scaler_models):\n",
    "    new_centers = []\n",
    "    for center in list_centers:\n",
    "        n_center = []\n",
    "        for c in range(len(center)):\n",
    "            n_center.append(scaler_models[c].denormalize(center[c]))\n",
    "        new_centers.append(n_center)\n",
    "    return new_centers\n",
    "\n",
    "\n",
    "#minmax\n",
    "# def denormalize_cluster(cluster_centers, list_scaler_models):\n",
    "#     r_centers = []\n",
    "#     for center in cluster_centers:\n",
    "#         r_center = []\n",
    "#         for index in range(len(center)):\n",
    "#             s_model = list_scaler_models[index]\n",
    "#             r_center.append(s_model.denormalize(center[index]))\n",
    "#         r_centers.append(r_center)\n",
    "#     return r_centers\n",
    "\n",
    "\n",
    "def denormalize_cluster(cluster_centers, list_scaler_models, columns):\n",
    "    r_centers = []\n",
    "    length = len(columns)\n",
    "    for center in cluster_centers:\n",
    "        r_center = []\n",
    "        print center\n",
    "        denormed_center = [0]*length\n",
    "        \n",
    "        #for each scaler models, get index in columns and denorm\n",
    "        for k,v in list_scaler_models.iteritems():\n",
    "            col_name = k\n",
    "            model = v\n",
    "            index = columns.index(col_name)\n",
    "            #print index, 'masa2'\n",
    "            denormed_center[index] = center[index]*model.std+model.mean\n",
    "        r_centers.append(denormed_center)\n",
    "        \n",
    "#         for index in range(len(center)):\n",
    "#             s_model = list_scaler_models[index]\n",
    "#             #model = s_model.mmModel\n",
    "#             denormed_val = center[index]*s_model.std+s_model.mean\n",
    "#             r_center.append(denormed_val)\n",
    "#         r_centers.append(r_center)\n",
    "    return r_centers\n",
    "\n",
    "def get_distribution_of_workouts(transformed_df):\n",
    "    #count of predictions\n",
    "    return transformed_df.groupBy('prediction').count().orderBy('prediction')\n",
    "\n",
    "#transform (add prediction column) to the dataframe based on the kmeamodel input\n",
    "def transform_df_with_kmeamodel(kmeanModel, df, cols_to_cluster, scale_dict={}):\n",
    "    #need to vectorize and normalize the df\n",
    "    df_to_transform, scalerModels = normalize_vectorize_df(big_sumstats, cols_to_cluster, scale_dict)\n",
    "    df_to_transform = create_feature_column(df_to_transform, cols_to_cluster)\n",
    "    return kmeanModel.transform(df_to_transform), scalerModels\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "def vectorizeData(data):\n",
    "    return data.rdd.map(lambda r: [r[0], r[1], r[2], Vectors.dense(r[3:6]), r[7]]).toDF([\\\n",
    "            'prediction', 'userid', 'workoutid', 'label', 'max_elapsed_time'])\n",
    "\n",
    "# total_df= fix_df(sumstats.join(prediction_df.select('workoutid','prediction'), \\\n",
    "#                                sumstats['workoutid']==prediction_df['workoutid'],'inner').\\\n",
    "#                             drop(prediction_df['workoutid']))\n",
    "\n",
    "def denorm_df_with_standarscaler(dict_scalerModels, df):\n",
    "    for k,v in dict_scalerModels.iteritems():\n",
    "        #print '--denorm ', k, '--'\n",
    "        col_name = k\n",
    "        scaler = v\n",
    "        mean = float(scaler.mean[0])\n",
    "        std = float(scaler.std[0])\n",
    "        denorm_udf = udf(lambda value: value*std + mean, FloatType())\n",
    "        df = df.withColumn(col_name, denorm_udf(df[col_name]))\n",
    "    return df\n",
    "    \n",
    "#     mean = float(scalerModel.mean[0])\n",
    "#     std = float(scalerModel.std[0])\n",
    "#     denorm_udf = udf(lambda value: value*std + mean, FloatType())\n",
    "#     return df.withColumn()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     def test(scaler, df):\n",
    "#     dist_scaler = scaler\n",
    "#     print df.describe().show(10,False)\n",
    "#     #nd = df.withColumn('geo_distance', dist_scaler.s_udf(df['geo_distance']))\n",
    "#     mean = float(dist_scaler.mean[0])\n",
    "#     std = float(dist_scaler.std[0])\n",
    "#     print mean,std\n",
    "#     denorm_udf = udf(lambda value: value*std + mean, FloatType())\n",
    "#     #denorm_udf = udf(lambda value: value*std, FloatType())\n",
    "\n",
    "#     #denorm_udf = udf(lambda x: 'new_value', StringType())\n",
    "\n",
    "#     r_df = df.withColumn('geo_distance', denorm_udf(df['geo_distance']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.71 ms\n"
     ]
    }
   ],
   "source": [
    "#magic\n",
    "unvector = udf(lambda value: value[0].item(), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[altitude: decimal(20,10), heart_rate: decimal(10,5), latitude: decimal(20,10), longitude: decimal(20,10), speed: decimal(20,10), workoutid: int, time: int, altitude_first: decimal(10,5), altitude_second: decimal(10,5), elapsed_time: int, speed_first: decimal(10,5), geo_distance: decimal(20,10), elapsed_distance: decimal(15,10), heart_rate_ma_25: decimal(8,5), speed_ma_100: decimal(8,5), speed_ma_50: decimal(8,5), speed_by_geo: decimal(20,10)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrameReader\n",
    "\n",
    "url = 'postgresql://34.199.246.37:5432/endomondo_sample'\n",
    "#url = 'postgresql://172.31.29.1:5432/endomondo'\n",
    "\n",
    "properties = {'user':'endomondo', 'password': 'End0m0ndo'}\n",
    "\n",
    "df_runs = DataFrameReader(sqlContext).jdbc(url='jdbc:%s' % url, table='run', properties=properties)\n",
    "\n",
    "df_users = DataFrameReader(sqlContext).jdbc(url='jdbc:%s' % url, table='run_by_workout', properties=properties)\n",
    "\n",
    "df_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "df_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.6 ms\n"
     ]
    }
   ],
   "source": [
    "sub_df = df_runs.limit(1000)\n",
    "df_users = df_users.withColumn('diff_altitude', \\\n",
    "                    df_users['altitude_max'] \\\n",
    "                    -df_users['altitude_min'])\n",
    "sumstats = df_users.repartition(30)\n",
    "#sumstats = df_users.limit(500).repartition(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|    diff_altitude|\n",
      "+-------+-----------------+\n",
      "|  count|            19671|\n",
      "|   mean|91.14937387423110|\n",
      "| stddev|444.8822654794946|\n",
      "|    min|            0E-10|\n",
      "|    max| 13103.6000000000|\n",
      "+-------+-----------------+\n",
      "\n",
      "time: 740 ms\n"
     ]
    }
   ],
   "source": [
    "sumstats.select('diff_altitude').describe().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.05 ms\n"
     ]
    }
   ],
   "source": [
    "route_cols = ['diff_altitude', 'geo_distance']\n",
    "#route_cols = ['diff_altitude']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster start\n",
      "vectorize done in: 0:00:00.051074\n",
      "------- normalize  diff_altitude -------\n",
      "0:00:00.368397\n",
      "0:00:00.051474\n",
      "------- normalize  geo_distance -------\n",
      "0:00:02.383062\n",
      "0:00:00.053614\n",
      "clustering in  0:00:38.390130\n",
      "done\n",
      "time: 41.3 s\n"
     ]
    }
   ],
   "source": [
    "scale_dict = {}\n",
    "\n",
    "r_obj = cluster_df(sumstats, route_cols, 5, init_var=True,\\\n",
    "                  scale_dict=scale_dict, bisectingKMean=True,\\\n",
    "                  debug=True)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|summary|       diff_altitude|        geo_distance|\n",
      "+-------+--------------------+--------------------+\n",
      "|  count|               24876|               24876|\n",
      "|   mean|-0.05221769265578752|-0.00732095255894...|\n",
      "| stddev| 0.22178324766198665| 0.17670662451058466|\n",
      "|    min|         -0.18093379|         -0.27459386|\n",
      "|    max|            2.992725|           2.9708807|\n",
      "+-------+--------------------+--------------------+\n",
      "\n",
      "None\n",
      "time: 5.36 s\n"
     ]
    }
   ],
   "source": [
    "print route_df.select(route_cols).describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|summary|       diff_altitude|        geo_distance|\n",
      "+-------+--------------------+--------------------+\n",
      "|  count|               24876|               24876|\n",
      "|   mean|-0.05221769265578752|-0.00732095255894...|\n",
      "| stddev| 0.22178324766198665| 0.17670662451058466|\n",
      "|    min|         -0.18093379|         -0.27459386|\n",
      "|    max|            2.992725|           2.9708807|\n",
      "+-------+--------------------+--------------------+\n",
      "\n",
      "None\n",
      "time: 5.7 s\n"
     ]
    }
   ],
   "source": [
    "print df_to_denorm.select(route_cols).describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.7 ms\n"
     ]
    }
   ],
   "source": [
    "route_kmodel = r_obj[0]\n",
    "route_df = r_obj[4]\n",
    "route_scalerModels = r_obj[2]\n",
    "df_to_denorm = r_obj[1]\n",
    "#route_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geo_distance StandardScaler_4853be56338659051f1c\n",
      "diff_altitude StandardScaler_466dbcd659afa3ccd900\n",
      "time: 1.51 ms\n"
     ]
    }
   ],
   "source": [
    "for k,v in route_scalerModels.iteritems():\n",
    "    print k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24876"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.85 s\n"
     ]
    }
   ],
   "source": [
    "route_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 685 µs\n"
     ]
    }
   ],
   "source": [
    "###check if routes make sense\n",
    "#route_df.select(route_cols).toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.16 s\n"
     ]
    }
   ],
   "source": [
    "route_prediction_df = route_kmodel.transform(route_df)\n",
    "#print route_prediction_df.count()\n",
    "route_prediction_cols = ['route_prediction','route_features']\n",
    "distinct_clusters = route_prediction_df.select('prediction').distinct().collect()\n",
    "route_prediction_df = route_prediction_df.withColumnRenamed('prediction', 'route_prediction')\n",
    "route_prediction_df = route_prediction_df.withColumnRenamed('features', 'route_features')\n",
    "\n",
    "route_prediction_df = denorm_df_with_standarscaler(route_scalerModels, route_prediction_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+--------------------+\n",
      "|summary|diff_altitude        |geo_distance        |\n",
      "+-------+---------------------+--------------------+\n",
      "|count  |24876                |24876               |\n",
      "|mean   |51.02150974717797    |0.13627124255003475 |\n",
      "|stddev |87.9122041826895     |0.09009529141855512 |\n",
      "|min    |1.8724292658589548E-6|4.432102684859274E-9|\n",
      "|max    |1258.0               |1.654731273651123   |\n",
      "+-------+---------------------+--------------------+\n",
      "\n",
      "time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "route_prediction_df.select(route_cols).describe().show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|route_prediction|count|\n",
      "+----------------+-----+\n",
      "|               1|  142|\n",
      "|               3| 4575|\n",
      "|               4| 2711|\n",
      "|               2|  630|\n",
      "|               0|16818|\n",
      "+----------------+-----+\n",
      "\n",
      "time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "route_prediction_df.groupBy('route_prediction').count().show()\n",
    "route_prediction_df = fix_df(route_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.12519179, -0.07004625]),\n",
       " array([ 1.95885868,  0.19198119]),\n",
       " array([ 0.66136054,  0.09302346]),\n",
       " array([ 0.07946549,  0.00195137]),\n",
       " array([-0.07879936,  0.33516802])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.84 ms\n"
     ]
    }
   ],
   "source": [
    "route_kmodel.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12519179 -0.07004625]\n",
      "[ 1.95885868  0.19198119]\n",
      "[ 0.66136054  0.09302346]\n",
      "[ 0.07946549  0.00195137]\n",
      "[-0.07879936  0.33516802]\n",
      "[[22.095457609561755, 0.1042902478691724], [848.1879279535751, 0.2378870411397166], [333.8753130262295, 0.18743264714643124], [103.21913428407004, 0.140998811351551], [40.48485885947855, 0.3108919976252643]]\n",
      "time: 58.1 ms\n"
     ]
    }
   ],
   "source": [
    "denormed_clusters = denormalize_cluster(route_kmodel.clusterCenters(),  route_scalerModels, route_cols)\n",
    "#save cluster centers for route\n",
    "#unvector = udf(lambda value: value[0].item(), FloatType())\n",
    "route_viz_cols = route_cols\n",
    "for i in range(len(denormed_clusters)):\n",
    "    for j in range(len(route_viz_cols)):\n",
    "        denormed_clusters[i][j] = float(denormed_clusters[i][j][0])\n",
    "print denormed_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 419 ms\n"
     ]
    }
   ],
   "source": [
    "sc.parallelize(denormed_clusters).toDF(route_viz_cols).toPandas().to_csv('route_clusters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "denormalize_cluster(route_kmodel.clusterCenters(),  route_scalerModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0| 7087|\n",
      "|         1|15051|\n",
      "|         2| 1730|\n",
      "|         3|  223|\n",
      "|         4|  785|\n",
      "+----------+-----+\n",
      "\n",
      "time: 5.37 s\n"
     ]
    }
   ],
   "source": [
    "distribution_df = get_distribution_of_workouts(route_kmodel.transform(route_df))\n",
    "distribution_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 782 µs\n"
     ]
    }
   ],
   "source": [
    "#######################end route checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 2, 0]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.02 ms\n"
     ]
    }
   ],
   "source": [
    "#get cluster numbers\n",
    "cluster_numbers = [int(distinct_clusters[i][0]) for i in range(len(distinct_clusters))]\n",
    "cluster_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.8 ms\n"
     ]
    }
   ],
   "source": [
    "#create list of dataframes for each route_prediction value\n",
    "list_route_clusters = [route_prediction_df.where(route_prediction_df['route_prediction']==i) \\\n",
    "                      for i in cluster_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'workoutid',\n",
       " 'userid',\n",
       " 'start_time',\n",
       " 'start_altitude',\n",
       " 'start_latitude',\n",
       " 'start_longitude',\n",
       " 'series_length',\n",
       " 'series_time_delta',\n",
       " 'series_time_delta_average',\n",
       " 'timezone',\n",
       " 'id',\n",
       " 'altitude_max',\n",
       " 'altitude_min',\n",
       " 'calories',\n",
       " 'distance',\n",
       " 'duration',\n",
       " 'heart_rate_avg',\n",
       " 'heart_rate_max',\n",
       " 'hydration',\n",
       " 'speed_avg',\n",
       " 'speed_max',\n",
       " 'humidity',\n",
       " 'temperature',\n",
       " 'wind_speed',\n",
       " 'elapsed_time',\n",
       " 'time_since_last_workout',\n",
       " 'diff_altitude',\n",
       " 'geo_distance',\n",
       " 'route_features',\n",
       " 'route_prediction']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.88 ms\n"
     ]
    }
   ],
   "source": [
    "list_route_clusters[0].schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-14092ea2453a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_route_clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diff_altitude'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'geo_distance'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'speed_avg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.14 s\n"
     ]
    }
   ],
   "source": [
    "list_route_clusters[0].select(['diff_altitude','geo_distance','speed_avg']).describe().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cluster start\n",
      "vectorize done in: 0:00:00.077889\n",
      "cluster done 0:00:24.236032\n",
      "elapsed_time StandardScaler_47c5b55aee435a1d6b88\n",
      "heart_rate_avg StandardScaler_415bb8b48b5acd79c8a9\n",
      "speed_avg StandardScaler_4d04b35cb69570330164\n",
      "transforming\n",
      "[-0.03976819 -0.51834702  0.02008771]\n",
      "[-6.52477884 -1.25541839  0.68115728]\n",
      "[ 0.43542948  1.16146665 -0.56828775]\n",
      "\n",
      "\n",
      "joined 0:00:00.129802\n",
      "1 cluster start\n",
      "vectorize done in: 0:00:00.077336\n",
      "cluster done 0:00:30.264793\n",
      "elapsed_time StandardScaler_47228aea13008b0d9267\n",
      "heart_rate_avg StandardScaler_4562a841079738c4f600\n",
      "speed_avg StandardScaler_411789b26ab1553c9d6c\n",
      "transforming\n",
      "[ 0.36744106  0.55491902 -0.02846549]\n",
      "[ 0.1783769  -0.82193714 -0.00316717]\n",
      "[-3.10485956  0.08690412 -0.01893489]\n",
      "\n",
      "\n",
      "joined 0:00:00.130059\n",
      "2 cluster start\n",
      "vectorize done in: 0:00:00.079202\n",
      "cluster done 0:00:30.192969\n",
      "elapsed_time StandardScaler_427f8541b3a567010616\n",
      "heart_rate_avg StandardScaler_480b8574051e8e3fddc2\n",
      "speed_avg StandardScaler_450fbcce2e79637f141a\n",
      "transforming\n",
      "[ 0.44779982  0.26275607 -0.1579571 ]\n",
      "[ 0.25668679 -0.52232457  0.06718192]\n",
      "[-2.68175884 -0.01638876 -0.04987722]\n",
      "\n",
      "\n",
      "joined 0:00:00.130785\n",
      "3 cluster start\n",
      "vectorize done in: 0:00:00.071890\n",
      "cluster done 0:00:29.897890\n",
      "elapsed_time StandardScaler_4ebca6b85a9bca91395a\n",
      "heart_rate_avg StandardScaler_4ae6b0faf21eac66997c\n",
      "speed_avg StandardScaler_4806beebdb8af5fae74f\n",
      "transforming\n",
      "[ 0.18212074 -0.80008376  0.51880864]\n",
      "[ 0.33193422  0.26277956 -0.36240987]\n",
      "[-3.28648686  0.28805982 -0.18885866]\n",
      "\n",
      "\n",
      "joined 0:00:00.129709\n",
      "4 cluster start\n",
      "vectorize done in: 0:00:00.079112\n",
      "cluster done 0:00:55.011593\n",
      "elapsed_time StandardScaler_46449978b1741a8f1e61\n",
      "heart_rate_avg StandardScaler_4fe2a7fdc9d2678ca5c0\n",
      "speed_avg StandardScaler_4a67a22965aee54f2bbc\n",
      "transforming\n",
      "[ 0.41589771  0.43035082 -0.01147476]\n",
      "[-2.60165746  0.16143236 -0.01134174]\n",
      "[ 0.21051928 -0.99213818 -0.01097817]\n",
      "\n",
      "\n",
      "joined 0:00:00.130209\n",
      "time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "#got list of dataframes by route cluster. for each of them, create new clusters \n",
    "#add a new column 'performance_prediction' for each cluster\n",
    "\n",
    "performance_cols_to_cluster = ['heart_rate_avg', 'speed_avg', 'elapsed_time']\n",
    "\n",
    "dict_cluster_to_objs = {}\n",
    "\n",
    "n_list_route_clusters = []\n",
    "\n",
    "perf_centers_df_denormed = []\n",
    "\n",
    "for i in range(len(list_route_clusters)):\n",
    "    print i, \n",
    "#     #cluster\n",
    "#     c_df = list_route_clusters[i]\n",
    "#     r_obj = cluster_df(c_df, performance_cols_to_cluster, 5, init_var=True, \\\n",
    "#                       scale_dict = scale_dict, bisectingKMean=True,\\\n",
    "#                       debug=False)\n",
    "#     dict_cluster_to_objs[i] = r_obj\n",
    "\n",
    "#     #transform the dataset\n",
    "#     p_kmodel = r_obj[0]\n",
    "#     p_df_to_transform = r_obj[1]\n",
    "#     p_scalerModels = r_obj[2]\n",
    "    \n",
    "#     p_kmodel.transform(p_df_to_transform)\n",
    "    c_df = list_route_clusters[i].repartition(20)\n",
    "    #print c_df.select(route_cols + performance_cols_to_cluster).describe().show(10,False)\n",
    "    #cluster within the route cluster dataframe\n",
    "    a = datetime.now()\n",
    "\n",
    "    r_obj = cluster_df(c_df, performance_cols_to_cluster, 3, init_var=True, \\\n",
    "                      scale_dict = scale_dict, bisectingKMean=True,\\\n",
    "                      debug=False)\n",
    "    \n",
    "    b = datetime.now()\n",
    "    print 'cluster done', b-a\n",
    "    dict_cluster_to_objs[i] = r_obj\n",
    "\n",
    "    #transform the dataset\n",
    "    p_kmodel = r_obj[0]\n",
    "    p_df_to_transform = r_obj[4]\n",
    "    p_scalerModels = r_obj[2]\n",
    "    for k,v in p_scalerModels.iteritems():\n",
    "        print k,v\n",
    "    print 'transforming'\n",
    "    temp_df = p_kmodel.transform(p_df_to_transform)\n",
    "    temp_df = temp_df.withColumnRenamed('features', 'perf_features')\n",
    "    temp_df = temp_df.withColumnRenamed('prediction', 'perf_prediction')\n",
    "    #print c_df.schema\n",
    "    \n",
    "    denormed_center = denormalize_cluster(p_kmodel.clusterCenters(),  p_scalerModels, performance_cols_to_cluster)\n",
    "    for i in range(len(denormed_center)):\n",
    "        for j in range(len(performance_cols_to_cluster)):\n",
    "            denormed_center[i][j] = float(denormed_center[i][j][0])\n",
    "    \n",
    "    perf_centers_df_denormed.append(\\\n",
    "                                    sc.parallelize(denormed_center)\\\n",
    "                                    .toDF(performance_cols_to_cluster))\n",
    "    \n",
    "    \n",
    "    print '\\n'\n",
    "    #transform\n",
    "    c_df = c_df.drop('perf_features')\n",
    "    c_df = c_df.drop('perf_prediction')\n",
    "    a = datetime.now()\n",
    "\n",
    "    #list_route_clusters[i] = c_df.join(temp_df.select('workoutid','perf_prediction', 'perf_features'),\\\n",
    "    #                                  temp_df['workoutid']==c_df['workoutid'], 'inner')\\\n",
    "    #                                    .drop(temp_df['workoutid'])\n",
    "#     n_list_route_clusters.append(c_df.join(temp_df.select('workoutid','perf_prediction', 'perf_features'),\\\n",
    "#                                       temp_df['workoutid']==c_df['workoutid'], 'inner')\\\n",
    "#                                         .drop(temp_df['workoutid']))\n",
    "    \n",
    "    #print r_obj[1].select(route_cols).describe().show(10,False)\n",
    "    denormed_df = denorm_df_with_standarscaler(p_scalerModels, r_obj[1])\n",
    "    \n",
    "    #denorm route info\n",
    "    #denormed_df = denorm_df_with_standarscaler(route_scalerModels, denormed_df)\n",
    "\n",
    "    \n",
    "    n_list_route_clusters.append(denormed_df.join(temp_df.select('workoutid','perf_prediction', 'perf_features'),\\\n",
    "                                      temp_df['workoutid']==denormed_df['workoutid'], 'inner')\\\n",
    "                                        .drop(temp_df['workoutid']))\n",
    "    \n",
    "    b = datetime.now()\n",
    "    print 'joined', b-a\n",
    "    \n",
    "    #print denormed_df.select(route_cols + performance_cols_to_cluster).describe().show(10,False)\n",
    "    #break\n",
    "    #list_route_clusters[i].show()\n",
    "    #break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|    diff_altitude|\n",
      "+-------+-----------------+\n",
      "|  count|            19671|\n",
      "|   mean|91.14937387423110|\n",
      "| stddev|444.8822654794946|\n",
      "|    min|            0E-10|\n",
      "|    max| 13103.6000000000|\n",
      "+-------+-----------------+\n",
      "\n",
      "time: 149 ms\n"
     ]
    }
   ],
   "source": [
    "sumstats.select(['diff_altitude']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.07 ms\n"
     ]
    }
   ],
   "source": [
    "new_features=[ 'workoutid',\\\n",
    " 'route_prediction',\\\n",
    " 'perf_prediction', 'userid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 133 ms\n"
     ]
    }
   ],
   "source": [
    "list_total_dfs = [i.select(new_features + route_cols + performance_cols_to_cluster) for i in n_list_route_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['workoutid',\n",
       " 'route_prediction',\n",
       " 'perf_prediction',\n",
       " 'userid',\n",
       " 'diff_altitude',\n",
       " 'geo_distance',\n",
       " 'heart_rate_avg',\n",
       " 'speed_avg',\n",
       " 'elapsed_time']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.71 ms\n"
     ]
    }
   ],
   "source": [
    "list_total_dfs[0].schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_total_dfs[0].select(cols_viz).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 872 µs\n"
     ]
    }
   ],
   "source": [
    "dd = list_total_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "#use total_df to find avg_spd/avg_dist for each user in each cluster\n",
    "# from the predictions, group by userid/clusterid and calculate avg speed and avg distance for each user\n",
    "\n",
    "w = Window.partitionBy('userid', 'route_prediction')\n",
    "\n",
    "new_speed = 'user_avg_speed'\n",
    "new_dist = 'user_avg_dist'\n",
    "\n",
    "for i in range(len(list_total_dfs)):\n",
    "    df = list_total_dfs[i].repartition(30)\n",
    "    df_speed = df.select('userid', 'route_prediction',\\\n",
    "                          avg('speed_avg').over(w).alias(new_speed)).distinct()\n",
    "    df_dist = df.select('userid', 'route_prediction',\\\n",
    "                          avg('geo_distance').over(w).alias(new_dist)).distinct()\n",
    "    total = df_speed.join(df_dist,(df_speed.route_prediction==df_dist.route_prediction)\\\n",
    "                           & (df_speed.userid==df_dist.userid),\\\n",
    "                     'inner').drop(df_speed.route_prediction).drop(df_speed.userid)\n",
    "    \n",
    "    \n",
    "    list_total_dfs[i] = df.join(total, total.userid==df.userid, \\\n",
    "                                'inner').drop(total.route_prediction)\\\n",
    "                                .drop(total.userid)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15012"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "list_total_dfs[0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.34 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cols_viz = ['diff_altitude', 'geo_distance', 'heart_rate_avg', 'speed_avg', 'user_avg_dist', 'user_avg_speed']\n",
    "\n",
    "#list_total_dfs[1].select(cols_viz).describe().show(truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------------+------+-------------+------------+--------------+---------+------------+--------------+-------------+\n",
      "|workoutid|route_prediction|perf_prediction|userid|diff_altitude|geo_distance|heart_rate_avg|speed_avg|elapsed_time|user_avg_speed|user_avg_dist|\n",
      "+---------+----------------+---------------+------+-------------+------------+--------------+---------+------------+--------------+-------------+\n",
      "+---------+----------------+---------------+------+-------------+------------+--------------+---------+------------+--------------+-------------+\n",
      "\n",
      "time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "###check avg\n",
    "#list_total_dfs[2].where(col('userid')==6601602).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 443 ms\n"
     ]
    }
   ],
   "source": [
    "#make one df from list_total_df\n",
    "new_concat_df = unionAll(*list_total_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24654"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "new_concat_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 166 ms\n"
     ]
    }
   ],
   "source": [
    "#dd = new_concat_df.select(cols_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 538 µs\n"
     ]
    }
   ],
   "source": [
    "#dd.describe().show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------+---------------+----------+-------------+------------+--------------+----------+------------+--------------+-------------+\n",
      "|summary| workoutid|route_prediction|perf_prediction|    userid|diff_altitude|geo_distance|heart_rate_avg| speed_avg|elapsed_time|user_avg_speed|user_avg_dist|\n",
      "+-------+----------+----------------+---------------+----------+-------------+------------+--------------+----------+------------+--------------+-------------+\n",
      "|  count|     24654|           24654|          24654|     24654|        24654|       24654|         24654|     24654|       24654|         24654|        24654|\n",
      "|   mean|3.63313...|      1.03139...|     0.68504...|4546589...|   50.6747...|  0.13424...|    131.244...|10.7855...|  4380.01...|    10.7855...|   0.13424...|\n",
      "| stddev|1.62107...|      1.54710...|     0.82563...|3802550...|   86.7839...|  0.08359...|    48.1253...|2.32509...|  9191.96...|    1.77780...|   0.06968...|\n",
      "|    min|     32946|               0|              0|        69|   1.87242...|  4.43210...|           0.0|-3.2287147|       501.0|           0.0|   4.43210...|\n",
      "|    max| 672564729|               4|              2|  15481421|       1258.0|  1.65473...|       239.937| 23.394915|   1108301.0|    23.3949...|   0.86386...|\n",
      "+-------+----------+----------------+---------------+----------+-------------+------------+--------------+----------+------------+--------------+-------------+\n",
      "\n",
      "time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "new_concat_df.describe().show(truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autotime.py\t\t\t\t   mmscaler_wrapper.py\n",
      "autotime.pyc\t\t\t\t   mmscaler_wrapper.pyc\n",
      "check_free_space.sh\t\t\t   mycert.pem\n",
      "check_memory.sh\t\t\t\t   mykey.key\n",
      "cluster_centers.csv\t\t\t   new_run2.csv\n",
      "cluster_centers_gt10.csv\t\t   notebooks\n",
      "dave2.csv\t\t\t\t   pypy\n",
      "dave_.csv\t\t\t\t   README.md\n",
      "derby.log\t\t\t\t   Regression \n",
      "f_cluster.csv\t\t\t\t   Regression 1st Iteration\n",
      "f_clusters4.csv\t\t\t\t   Regression_2nd_Iteration\n",
      "f_clusters5.csv\t\t\t\t   Regression_3rd_Iteration\n",
      "f_clusters6.csv\t\t\t\t   route_clusters.csv\n",
      "import_influx.py\t\t\t   route_perf_clusters.csv\n",
      "Jason-Loading-Visualizing-Endomondo.ipynb  run_data_with_hr_spd.csv\n",
      "Larger Cluster Regression.ipynb\t\t   run_with_hr_users.csv\n",
      "masa-cluster series.ipynb\t\t   save-package-versions\n",
      "masa-cluster summary.ipynb\t\t   sc_test.ipynb\n",
      "masa-newclustering-Copy1.ipynb\t\t   sql_scripts\n",
      "masa-newclustering.ipynb\t\t   test.csv\n",
      "metastore_db.old\t\t\t   visualize_cluster.ipynb\n",
      "mmscaler_wrapper.ipynb\t\t\t   visualize_clusters.html\n",
      "time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "new_concat_df.toPandas().to_csv('dave_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/momori/dse/maomori/thesis/git/fitness_capstone\n",
      "time: 125 ms\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+-----+\n",
      "|route_prediction|perf_prediction|count|\n",
      "+----------------+---------------+-----+\n",
      "|               1|              1| 8764|\n",
      "|               3|              1|   77|\n",
      "|               3|              0|  125|\n",
      "|               2|              0| 1582|\n",
      "|               0|              0| 3100|\n",
      "|               0|              1|  788|\n",
      "|               1|              2| 1688|\n",
      "|               2|              2|   62|\n",
      "|               2|              1|   70|\n",
      "|               4|              2|  617|\n",
      "|               4|              1|   68|\n",
      "|               3|              2|   10|\n",
      "|               1|              0| 4560|\n",
      "|               0|              2| 3106|\n",
      "|               4|              0|   77|\n",
      "+----------------+---------------+-----+\n",
      "\n",
      "time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "new_concat_df.groupby(['route_prediction','perf_prediction'])\\\n",
    "    .count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|speed_avg         |\n",
      "+-------+------------------+\n",
      "|count  |24654             |\n",
      "|mean   |10.785569427380436|\n",
      "|stddev |2.3250990751402534|\n",
      "|min    |-3.2287147        |\n",
      "|max    |23.394915         |\n",
      "+-------+------------------+\n",
      "\n",
      "time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "new_concat_df.select('speed_avg').describe().show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 809 µs\n"
     ]
    }
   ],
   "source": [
    "prediction_columns = ['route_prediction', 'perf_prediction']\n",
    "#frames = [df.toPandas() for df in n_list_route_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 543 µs\n"
     ]
    }
   ],
   "source": [
    "#new_concat_df = sqlContext.createDataFrame(pd.concat(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perf_clusters0.csv\n",
      "+-----------------+------------------+-----------------+\n",
      "|   heart_rate_avg|         speed_avg|     elapsed_time|\n",
      "+-----------------+------------------+-----------------+\n",
      "|134.4350000154713|11.513321991868393|6562.666684570312|\n",
      "+-----------------+------------------+-----------------+\n",
      "\n",
      "None\n",
      "perf_clusters1.csv\n",
      "+-----------------+-----------------+-----------------+\n",
      "|   heart_rate_avg|        speed_avg|     elapsed_time|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|162.6259991907229|15.18740863941769|6259.999990805761|\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "None\n",
      "time: 62.4 ms\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for i in perf_centers_df_denormed:\n",
    "    name='perf_clusters'+str(index)+'.csv'\n",
    "    print name\n",
    "    i.toPandas().to_csv(name)\n",
    "    print i.show()\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.8 ms\n"
     ]
    }
   ],
   "source": [
    "new_concat_df = unionAll(*n_list_route_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+-----+\n",
      "|route_prediction|perf_prediction|count|\n",
      "+----------------+---------------+-----+\n",
      "|               1|              0|    3|\n",
      "|               3|              0|    1|\n",
      "+----------------+---------------+-----+\n",
      "\n",
      "time: 3.88 s\n"
     ]
    }
   ],
   "source": [
    "new_concat_df.groupBy(['route_prediction','perf_prediction']).count()\\\n",
    ".orderBy(['route_prediction', 'perf_prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 65.5 ms\n"
     ]
    }
   ],
   "source": [
    "#remove small clusters, might be outliers\n",
    "new_concat_df = new_concat_df.where(\n",
    "    ((new_concat_df['route_prediction']!= 4) | (new_concat_df['perf_prediction']!= 0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['route_prediction', 'perf_prediction', 'diff_altitude', 'geo_distance', 'userid', 'workoutid', 'heart_rate_avg', 'speed_avg', 'elapsed_time']\n",
      "time: 1.16 ms\n"
     ]
    }
   ],
   "source": [
    "select_cols = prediction_columns+route_cols+['userid','workoutid']+performance_cols_to_cluster\n",
    "print select_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 720 µs\n"
     ]
    }
   ],
   "source": [
    "sum_cols = route_cols+performance_cols_to_cluster+['workoutid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+------------------+------------------+--------+---------+--------------------+--------------------+------------+\n",
      "|route_prediction|perf_prediction|     diff_altitude|      geo_distance|  userid|workoutid|      heart_rate_avg|           speed_avg|elapsed_time|\n",
      "+----------------+---------------+------------------+------------------+--------+---------+--------------------+--------------------+------------+\n",
      "|               1|              0|0.2209971696138382|0.7775065898895264|  431375|608308965|141.0000000000000...|12.01837677640000...|        6493|\n",
      "|               1|              0|0.2412315011024475| 0.919242799282074| 9286593|367180538|122.3050000000000...|11.40694893660000...|        6317|\n",
      "|               1|              0|0.2264968752861023|0.7706879377365112| 6828710|475598541|140.0000000000000...|11.11464029460000...|        6878|\n",
      "|               3|              0|0.6398912072181702|1.7554370164871216|13276532|376398864|162.6260000000000...|15.18740872950000...|        6260|\n",
      "+----------------+---------------+------------------+------------------+--------+---------+--------------------+--------------------+------------+\n",
      "\n",
      "time: 2.86 s\n"
     ]
    }
   ],
   "source": [
    "new_concat_df.select(select_cols).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 75.4 ms\n"
     ]
    }
   ],
   "source": [
    "f_cluster_df = new_concat_df.select(select_cols)\n",
    "f_cluster_df = f_cluster_df.withColumn('diff_altitude', unvector(f_cluster_df['diff_altitude']))\n",
    "f_cluster_df = f_cluster_df.withColumn('geo_distance', unvector(f_cluster_df['geo_distance']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.3 s\n"
     ]
    }
   ],
   "source": [
    "new_df = fix_df(new_concat_df.select(select_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.44 s\n"
     ]
    }
   ],
   "source": [
    "viz_selection_route = ['geo_distance', 'diff_altitude']\n",
    "\n",
    "new_df.select(viz_selection_route).toPandas().to_csv('f_clusters6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        geo_distance|       diff_altitude|\n",
      "+--------------------+--------------------+\n",
      "|-0.23650439083576202|-0.18164116144180298|\n",
      "| -0.2968972623348236| 0.23747123777866364|\n",
      "| -0.2613212466239929|-0.11914194375276566|\n",
      "| -0.2013634741306305| 0.12644322216510773|\n",
      "|-0.24943852424621582|-0.10811267048120499|\n",
      "|-0.26658928394317627|-0.18164116144180298|\n",
      "|-0.37416771054267883|  0.3808518052101135|\n",
      "| -0.2730995714664459|-0.11473023146390915|\n",
      "|-0.28780242800712585|-0.09703560173511505|\n",
      "|-0.23023466765880585|-0.13568584620952606|\n",
      "| -0.1474791318178177| -0.1125243753194809|\n",
      "| -0.1570734828710556|-0.03237832337617874|\n",
      "|-0.04410969838500023| 0.47202712297439575|\n",
      "|-0.14297319948673248|-0.10664209723472595|\n",
      "|-0.15701673924922943|-0.18164116144180298|\n",
      "| -0.1453741192817688|-0.13458292186260223|\n",
      "|-0.28630220890045166| 0.12864907085895538|\n",
      "|-0.15556730329990387|-0.02943718247115612|\n",
      "|-0.22201518714427948|-0.13642114400863647|\n",
      "|-0.24090249836444855|  0.3514403998851776|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "time: 360 ms\n"
     ]
    }
   ],
   "source": [
    "new_df.select(viz_selection_route).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19 s\n"
     ]
    }
   ],
   "source": [
    "new_df.write.csv('f_clusters7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2274.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 23 in stage 2376.0 failed 4 times, most recent failure: Lost task 23.3 in stage 2376.0 (TID 145990, 172.31.42.143, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-54-6c52063ef3db>\", line 2, in <lambda>\nTypeError: 'float' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2745)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2742)\n\tat sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-54-6c52063ef3db>\", line 2, in <lambda>\nTypeError: 'float' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-b1838449548b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf_cluster_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f_clusters5.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1574\u001b[0m         \"\"\"\n\u001b[1;32m   1575\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;31m##########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \"\"\"\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2274.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 23 in stage 2376.0 failed 4 times, most recent failure: Lost task 23.3 in stage 2376.0 (TID 145990, 172.31.42.143, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-54-6c52063ef3db>\", line 2, in <lambda>\nTypeError: 'float' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2745)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2742)\n\tat sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-54-6c52063ef3db>\", line 2, in <lambda>\nTypeError: 'float' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "f_cluster_df.toPandas().to_csv('f_clusters5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2275.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:579)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 2375.0 failed 4 times, most recent failure: Lost task 9.3 in stage 2375.0 (TID 145899, 172.31.42.143, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-54-6c52063ef3db>\", line 2, in <lambda>\nTypeError: 'float' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127)\n\t... 31 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-54-6c52063ef3db>\", line 2, in <lambda>\nTypeError: 'float' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-e7b805800940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf_cluster_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f_clusters5.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat)\u001b[0m\n\u001b[1;32m    709\u001b[0m                        \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnullValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescapeQuotes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mescapeQuotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoteAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquoteAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                        dateFormat=dateFormat, timestampFormat=timestampFormat)\n\u001b[0;32m--> 711\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2275.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:579)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 2375.0 failed 4 times, most recent failure: Lost task 9.3 in stage 2375.0 (TID 145899, 172.31.42.143, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-54-6c52063ef3db>\", line 2, in <lambda>\nTypeError: 'float' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127)\n\t... 31 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 70, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-54-6c52063ef3db>\", line 2, in <lambda>\nTypeError: 'float' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.33 s\n"
     ]
    }
   ],
   "source": [
    "f_cluster_df.write.csv('f_clusters5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_prediction</th>\n",
       "      <th>perf_prediction</th>\n",
       "      <th>diff_altitude</th>\n",
       "      <th>geo_distance</th>\n",
       "      <th>userid</th>\n",
       "      <th>workoutid</th>\n",
       "      <th>elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.606000</td>\n",
       "      <td>1.231000</td>\n",
       "      <td>0.005290</td>\n",
       "      <td>0.035301</td>\n",
       "      <td>4.742135e+06</td>\n",
       "      <td>4.056705e+08</td>\n",
       "      <td>3622.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.102715</td>\n",
       "      <td>0.745785</td>\n",
       "      <td>0.032204</td>\n",
       "      <td>0.040235</td>\n",
       "      <td>3.977771e+06</td>\n",
       "      <td>1.524253e+08</td>\n",
       "      <td>3754.103925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.900000e+01</td>\n",
       "      <td>9.894440e+05</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019071</td>\n",
       "      <td>1.607905e+06</td>\n",
       "      <td>3.126698e+08</td>\n",
       "      <td>2134.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.031582</td>\n",
       "      <td>3.575734e+06</td>\n",
       "      <td>4.162176e+08</td>\n",
       "      <td>3093.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.044650</td>\n",
       "      <td>7.482039e+06</td>\n",
       "      <td>5.252203e+08</td>\n",
       "      <td>4217.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.548142e+07</td>\n",
       "      <td>6.551612e+08</td>\n",
       "      <td>84174.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       route_prediction  perf_prediction  diff_altitude  geo_distance  \\\n",
       "count       1000.000000      1000.000000    1000.000000   1000.000000   \n",
       "mean           1.606000         1.231000       0.005290      0.035301   \n",
       "std            1.102715         0.745785       0.032204      0.040235   \n",
       "min            0.000000         0.000000       0.000000      0.000000   \n",
       "25%            1.000000         1.000000       0.000000      0.019071   \n",
       "50%            2.000000         1.000000       0.002490      0.031582   \n",
       "75%            3.000000         2.000000       0.005310      0.044650   \n",
       "max            3.000000         2.000000       1.000000      1.000000   \n",
       "\n",
       "             userid     workoutid  elapsed_time  \n",
       "count  1.000000e+03  1.000000e+03   1000.000000  \n",
       "mean   4.742135e+06  4.056705e+08   3622.448000  \n",
       "std    3.977771e+06  1.524253e+08   3754.103925  \n",
       "min    6.900000e+01  9.894440e+05    506.000000  \n",
       "25%    1.607905e+06  3.126698e+08   2134.500000  \n",
       "50%    3.575734e+06  4.162176e+08   3093.000000  \n",
       "75%    7.482039e+06  5.252203e+08   4217.500000  \n",
       "max    1.548142e+07  6.551612e+08  84174.000000  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 282 ms\n"
     ]
    }
   ],
   "source": [
    "f_cluster_df.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.56 s\n"
     ]
    }
   ],
   "source": [
    "f_cluster_df.write.csv('f_cluster2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.41 s\n"
     ]
    }
   ],
   "source": [
    "f_cluster_df.write.parquet('f_cluster3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.1 ms\n"
     ]
    }
   ],
   "source": [
    "schema=f_cluster_df.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Unable to infer schema for Parquet. It must be specified manually.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-2dd5d4b21c0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f_cluster3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \"\"\"\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Unable to infer schema for Parquet. It must be specified manually.;'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 75 ms\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.parquet('f_cluster3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.56 s\n"
     ]
    }
   ],
   "source": [
    "f_cluster_df.toPandas().to_csv('f_clusters4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>route_prediction</th>\n",
       "      <th>perf_prediction</th>\n",
       "      <th>diff_altitude</th>\n",
       "      <th>geo_distance</th>\n",
       "      <th>userid</th>\n",
       "      <th>workoutid</th>\n",
       "      <th>heart_rate_avg</th>\n",
       "      <th>speed_avg</th>\n",
       "      <th>elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050324</td>\n",
       "      <td>43462</td>\n",
       "      <td>22952825</td>\n",
       "      <td>179.000</td>\n",
       "      <td>9.931427</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.049722</td>\n",
       "      <td>1038205</td>\n",
       "      <td>29839134</td>\n",
       "      <td>176.577</td>\n",
       "      <td>14.940946</td>\n",
       "      <td>1016534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004915</td>\n",
       "      <td>0.049018</td>\n",
       "      <td>114073</td>\n",
       "      <td>33675191</td>\n",
       "      <td>168.000</td>\n",
       "      <td>14.060335</td>\n",
       "      <td>25389273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.054654</td>\n",
       "      <td>3276737</td>\n",
       "      <td>56085342</td>\n",
       "      <td>162.796</td>\n",
       "      <td>10.481048</td>\n",
       "      <td>14255297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.047487</td>\n",
       "      <td>324779</td>\n",
       "      <td>60963533</td>\n",
       "      <td>155.061</td>\n",
       "      <td>9.609221</td>\n",
       "      <td>15717111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>860533</td>\n",
       "      <td>97645888</td>\n",
       "      <td>164.549</td>\n",
       "      <td>11.894138</td>\n",
       "      <td>2190253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004655</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>2255761</td>\n",
       "      <td>100089344</td>\n",
       "      <td>151.000</td>\n",
       "      <td>10.962511</td>\n",
       "      <td>18987867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018911</td>\n",
       "      <td>0.061267</td>\n",
       "      <td>81753</td>\n",
       "      <td>156154748</td>\n",
       "      <td>159.129</td>\n",
       "      <td>10.201762</td>\n",
       "      <td>112583114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049694</td>\n",
       "      <td>5325166</td>\n",
       "      <td>172545666</td>\n",
       "      <td>147.000</td>\n",
       "      <td>8.590226</td>\n",
       "      <td>22200235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.055214</td>\n",
       "      <td>983950</td>\n",
       "      <td>181978674</td>\n",
       "      <td>146.923</td>\n",
       "      <td>13.294408</td>\n",
       "      <td>63173301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.050270</td>\n",
       "      <td>3055418</td>\n",
       "      <td>205641648</td>\n",
       "      <td>148.103</td>\n",
       "      <td>12.646851</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006487</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>4007546</td>\n",
       "      <td>217545028</td>\n",
       "      <td>148.947</td>\n",
       "      <td>10.807348</td>\n",
       "      <td>21332383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049694</td>\n",
       "      <td>7516129</td>\n",
       "      <td>231192788</td>\n",
       "      <td>120.000</td>\n",
       "      <td>12.464891</td>\n",
       "      <td>25786842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.049942</td>\n",
       "      <td>2568526</td>\n",
       "      <td>245267463</td>\n",
       "      <td>161.000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5467964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.049743</td>\n",
       "      <td>8632256</td>\n",
       "      <td>278967805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.609412</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.058381</td>\n",
       "      <td>7123482</td>\n",
       "      <td>307957840</td>\n",
       "      <td>162.000</td>\n",
       "      <td>9.694527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058614</td>\n",
       "      <td>2046947</td>\n",
       "      <td>323189037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.500258</td>\n",
       "      <td>74726938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052808</td>\n",
       "      <td>14544270</td>\n",
       "      <td>324052089</td>\n",
       "      <td>162.000</td>\n",
       "      <td>10.433578</td>\n",
       "      <td>2120257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.055774</td>\n",
       "      <td>8662931</td>\n",
       "      <td>334228990</td>\n",
       "      <td>150.917</td>\n",
       "      <td>13.797078</td>\n",
       "      <td>10636094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.049942</td>\n",
       "      <td>1481127</td>\n",
       "      <td>341948953</td>\n",
       "      <td>124.000</td>\n",
       "      <td>12.047904</td>\n",
       "      <td>116859704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.055724</td>\n",
       "      <td>3324472</td>\n",
       "      <td>344828496</td>\n",
       "      <td>153.083</td>\n",
       "      <td>16.196518</td>\n",
       "      <td>41686324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050060</td>\n",
       "      <td>7967148</td>\n",
       "      <td>357292847</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.995595</td>\n",
       "      <td>43442345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005892</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>1601558</td>\n",
       "      <td>385182528</td>\n",
       "      <td>150.000</td>\n",
       "      <td>11.404743</td>\n",
       "      <td>79374806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.049375</td>\n",
       "      <td>8221442</td>\n",
       "      <td>413323359</td>\n",
       "      <td>136.499</td>\n",
       "      <td>10.163268</td>\n",
       "      <td>78623135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052297</td>\n",
       "      <td>13165320</td>\n",
       "      <td>414704172</td>\n",
       "      <td>144.000</td>\n",
       "      <td>13.556877</td>\n",
       "      <td>195609592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050496</td>\n",
       "      <td>6828710</td>\n",
       "      <td>438993545</td>\n",
       "      <td>134.000</td>\n",
       "      <td>8.811808</td>\n",
       "      <td>54115776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003358</td>\n",
       "      <td>0.049682</td>\n",
       "      <td>751448</td>\n",
       "      <td>448951587</td>\n",
       "      <td>147.000</td>\n",
       "      <td>10.854042</td>\n",
       "      <td>158112304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>0.053413</td>\n",
       "      <td>907015</td>\n",
       "      <td>457325097</td>\n",
       "      <td>127.003</td>\n",
       "      <td>10.963228</td>\n",
       "      <td>124599456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.050199</td>\n",
       "      <td>4264744</td>\n",
       "      <td>474206692</td>\n",
       "      <td>141.783</td>\n",
       "      <td>11.081486</td>\n",
       "      <td>63799984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.057764</td>\n",
       "      <td>3702346</td>\n",
       "      <td>482380491</td>\n",
       "      <td>149.000</td>\n",
       "      <td>10.170138</td>\n",
       "      <td>136925610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24612</th>\n",
       "      <td>24612</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.025870</td>\n",
       "      <td>5407179</td>\n",
       "      <td>293806370</td>\n",
       "      <td>174.855</td>\n",
       "      <td>9.218582</td>\n",
       "      <td>15595756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24613</th>\n",
       "      <td>24613</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>136171</td>\n",
       "      <td>318781800</td>\n",
       "      <td>116.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97714109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24614</th>\n",
       "      <td>24614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042333</td>\n",
       "      <td>875281</td>\n",
       "      <td>331643924</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.051783</td>\n",
       "      <td>7094829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24615</th>\n",
       "      <td>24615</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.024852</td>\n",
       "      <td>11884453</td>\n",
       "      <td>343264984</td>\n",
       "      <td>162.646</td>\n",
       "      <td>12.772590</td>\n",
       "      <td>24274934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24616</th>\n",
       "      <td>24616</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.042406</td>\n",
       "      <td>10568980</td>\n",
       "      <td>362999773</td>\n",
       "      <td>160.000</td>\n",
       "      <td>12.081657</td>\n",
       "      <td>21943668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24617</th>\n",
       "      <td>24617</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.042350</td>\n",
       "      <td>1342020</td>\n",
       "      <td>389685109</td>\n",
       "      <td>160.247</td>\n",
       "      <td>11.683026</td>\n",
       "      <td>138804482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24618</th>\n",
       "      <td>24618</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.044034</td>\n",
       "      <td>13705737</td>\n",
       "      <td>395730773</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.336463</td>\n",
       "      <td>16438472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24619</th>\n",
       "      <td>24619</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.025554</td>\n",
       "      <td>3630588</td>\n",
       "      <td>405364522</td>\n",
       "      <td>150.000</td>\n",
       "      <td>9.681834</td>\n",
       "      <td>12861001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24620</th>\n",
       "      <td>24620</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.029815</td>\n",
       "      <td>14204020</td>\n",
       "      <td>412533726</td>\n",
       "      <td>137.940</td>\n",
       "      <td>8.055731</td>\n",
       "      <td>19949857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24621</th>\n",
       "      <td>24621</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040221</td>\n",
       "      <td>7430612</td>\n",
       "      <td>424658051</td>\n",
       "      <td>144.000</td>\n",
       "      <td>6.390744</td>\n",
       "      <td>36704012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24622</th>\n",
       "      <td>24622</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.038236</td>\n",
       "      <td>10917968</td>\n",
       "      <td>436534436</td>\n",
       "      <td>141.000</td>\n",
       "      <td>11.553000</td>\n",
       "      <td>24609522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24623</th>\n",
       "      <td>24623</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.026247</td>\n",
       "      <td>14066703</td>\n",
       "      <td>440991836</td>\n",
       "      <td>110.000</td>\n",
       "      <td>5.573852</td>\n",
       "      <td>17878312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24624</th>\n",
       "      <td>24624</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033659</td>\n",
       "      <td>3055418</td>\n",
       "      <td>446480438</td>\n",
       "      <td>167.000</td>\n",
       "      <td>12.173566</td>\n",
       "      <td>46542590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24625</th>\n",
       "      <td>24625</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006258</td>\n",
       "      <td>0.045299</td>\n",
       "      <td>2071522</td>\n",
       "      <td>453823384</td>\n",
       "      <td>138.000</td>\n",
       "      <td>8.263177</td>\n",
       "      <td>53224152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24626</th>\n",
       "      <td>24626</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.037828</td>\n",
       "      <td>5407179</td>\n",
       "      <td>461520942</td>\n",
       "      <td>136.746</td>\n",
       "      <td>9.130300</td>\n",
       "      <td>45753260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24627</th>\n",
       "      <td>24627</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.033055</td>\n",
       "      <td>10177904</td>\n",
       "      <td>463509097</td>\n",
       "      <td>132.947</td>\n",
       "      <td>7.000633</td>\n",
       "      <td>52514019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24628</th>\n",
       "      <td>24628</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>3091798</td>\n",
       "      <td>471653292</td>\n",
       "      <td>149.000</td>\n",
       "      <td>11.752661</td>\n",
       "      <td>69203011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24629</th>\n",
       "      <td>24629</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.046218</td>\n",
       "      <td>10028940</td>\n",
       "      <td>483490146</td>\n",
       "      <td>140.000</td>\n",
       "      <td>9.735862</td>\n",
       "      <td>45197458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24630</th>\n",
       "      <td>24630</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>0.033989</td>\n",
       "      <td>431375</td>\n",
       "      <td>495353301</td>\n",
       "      <td>150.000</td>\n",
       "      <td>12.981242</td>\n",
       "      <td>142720359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24631</th>\n",
       "      <td>24631</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.033755</td>\n",
       "      <td>818925</td>\n",
       "      <td>501915815</td>\n",
       "      <td>158.807</td>\n",
       "      <td>10.226917</td>\n",
       "      <td>67384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24632</th>\n",
       "      <td>24632</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045333</td>\n",
       "      <td>3235195</td>\n",
       "      <td>531829036</td>\n",
       "      <td>138.668</td>\n",
       "      <td>11.263830</td>\n",
       "      <td>83035204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24633</th>\n",
       "      <td>24633</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.045869</td>\n",
       "      <td>10057948</td>\n",
       "      <td>549001668</td>\n",
       "      <td>136.000</td>\n",
       "      <td>8.965123</td>\n",
       "      <td>29883418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24634</th>\n",
       "      <td>24634</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018178</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>1058434</td>\n",
       "      <td>586404536</td>\n",
       "      <td>169.000</td>\n",
       "      <td>9.196492</td>\n",
       "      <td>49414127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24635</th>\n",
       "      <td>24635</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.032048</td>\n",
       "      <td>6796415</td>\n",
       "      <td>595027908</td>\n",
       "      <td>133.000</td>\n",
       "      <td>12.161567</td>\n",
       "      <td>50709917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24636</th>\n",
       "      <td>24636</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>0.033409</td>\n",
       "      <td>4164701</td>\n",
       "      <td>595733242</td>\n",
       "      <td>141.065</td>\n",
       "      <td>9.925042</td>\n",
       "      <td>45610756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24637</th>\n",
       "      <td>24637</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.035668</td>\n",
       "      <td>2692363</td>\n",
       "      <td>619012388</td>\n",
       "      <td>145.000</td>\n",
       "      <td>10.550808</td>\n",
       "      <td>94402472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24638</th>\n",
       "      <td>24638</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045722</td>\n",
       "      <td>6716410</td>\n",
       "      <td>621586730</td>\n",
       "      <td>167.000</td>\n",
       "      <td>10.261841</td>\n",
       "      <td>44348845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24639</th>\n",
       "      <td>24639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.013025</td>\n",
       "      <td>885462</td>\n",
       "      <td>631384733</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.426982</td>\n",
       "      <td>97302610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24640</th>\n",
       "      <td>24640</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.033549</td>\n",
       "      <td>5386323</td>\n",
       "      <td>637964005</td>\n",
       "      <td>97.000</td>\n",
       "      <td>7.560915</td>\n",
       "      <td>50130575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24641</th>\n",
       "      <td>24641</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.026687</td>\n",
       "      <td>9336459</td>\n",
       "      <td>638057350</td>\n",
       "      <td>128.533</td>\n",
       "      <td>12.437240</td>\n",
       "      <td>56859618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24642 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  route_prediction  perf_prediction  diff_altitude  \\\n",
       "0               0                 1                2       0.000000   \n",
       "1               1                 1                2       0.003118   \n",
       "2               2                 1                2       0.004915   \n",
       "3               3                 1                2       0.002494   \n",
       "4               4                 1                2       0.011665   \n",
       "5               5                 1                2       0.000000   \n",
       "6               6                 1                2       0.004655   \n",
       "7               7                 1                1       0.018911   \n",
       "8               8                 1                2       0.000000   \n",
       "9               9                 1                1       0.002854   \n",
       "10             10                 1                2       0.000901   \n",
       "11             11                 1                2       0.006487   \n",
       "12             12                 1                2       0.000000   \n",
       "13             13                 1                2       0.002289   \n",
       "14             14                 1                0       0.001588   \n",
       "15             15                 1                2       0.002854   \n",
       "16             16                 1                0       0.000000   \n",
       "17             17                 1                2       0.000000   \n",
       "18             18                 1                2       0.001877   \n",
       "19             19                 1                1       0.002274   \n",
       "20             20                 1                2       0.002976   \n",
       "21             21                 1                0       0.000000   \n",
       "22             22                 1                1       0.005892   \n",
       "23             23                 1                1       0.002662   \n",
       "24             24                 1                1       0.000000   \n",
       "25             25                 1                2       0.000000   \n",
       "26             26                 1                1       0.003358   \n",
       "27             27                 1                1       0.003633   \n",
       "28             28                 1                1       0.001755   \n",
       "29             29                 1                1       0.004899   \n",
       "...           ...               ...              ...            ...   \n",
       "24612       24612                 0                1       0.001175   \n",
       "24613       24613                 0                2       0.000321   \n",
       "24614       24614                 0                0       0.000000   \n",
       "24615       24615                 0                1       0.001206   \n",
       "24616       24616                 0                1       0.001587   \n",
       "24617       24617                 0                2       0.002289   \n",
       "24618       24618                 0                0       0.000794   \n",
       "24619       24619                 0                1       0.000992   \n",
       "24620       24620                 0                1       0.000321   \n",
       "24621       24621                 0                1       0.000000   \n",
       "24622       24622                 0                1       0.001252   \n",
       "24623       24623                 0                1       0.001709   \n",
       "24624       24624                 0                1       0.000000   \n",
       "24625       24625                 0                1       0.006258   \n",
       "24626       24626                 0                1       0.001572   \n",
       "24627       24627                 0                1       0.001007   \n",
       "24628       24628                 0                2       0.002808   \n",
       "24629       24629                 0                1       0.001542   \n",
       "24630       24630                 0                2       0.004197   \n",
       "24631       24631                 0                2       0.002106   \n",
       "24632       24632                 0                2       0.000000   \n",
       "24633       24633                 0                1       0.000504   \n",
       "24634       24634                 0                1       0.018178   \n",
       "24635       24635                 0                1       0.004594   \n",
       "24636       24636                 0                1       0.006716   \n",
       "24637       24637                 0                2       0.000839   \n",
       "24638       24638                 0                1       0.000000   \n",
       "24639       24639                 0                0       0.002885   \n",
       "24640       24640                 0                1       0.004121   \n",
       "24641       24641                 0                2       0.001175   \n",
       "\n",
       "       geo_distance    userid  workoutid  heart_rate_avg  speed_avg  \\\n",
       "0          0.050324     43462   22952825         179.000   9.931427   \n",
       "1          0.049722   1038205   29839134         176.577  14.940946   \n",
       "2          0.049018    114073   33675191         168.000  14.060335   \n",
       "3          0.054654   3276737   56085342         162.796  10.481048   \n",
       "4          0.047487    324779   60963533         155.061   9.609221   \n",
       "5          0.050781    860533   97645888         164.549  11.894138   \n",
       "6          0.054519   2255761  100089344         151.000  10.962511   \n",
       "7          0.061267     81753  156154748         159.129  10.201762   \n",
       "8          0.049694   5325166  172545666         147.000   8.590226   \n",
       "9          0.055214    983950  181978674         146.923  13.294408   \n",
       "10         0.050270   3055418  205641648         148.103  12.646851   \n",
       "11         0.050633   4007546  217545028         148.947  10.807348   \n",
       "12         0.049694   7516129  231192788         120.000  12.464891   \n",
       "13         0.049942   2568526  245267463         161.000  12.000000   \n",
       "14         0.049743   8632256  278967805           0.000  10.609412   \n",
       "15         0.058381   7123482  307957840         162.000   9.694527   \n",
       "16         0.058614   2046947  323189037           0.000  10.500258   \n",
       "17         0.052808  14544270  324052089         162.000  10.433578   \n",
       "18         0.055774   8662931  334228990         150.917  13.797078   \n",
       "19         0.049942   1481127  341948953         124.000  12.047904   \n",
       "20         0.055724   3324472  344828496         153.083  16.196518   \n",
       "21         0.050060   7967148  357292847           0.000  16.995595   \n",
       "22         0.049729   1601558  385182528         150.000  11.404743   \n",
       "23         0.049375   8221442  413323359         136.499  10.163268   \n",
       "24         0.052297  13165320  414704172         144.000  13.556877   \n",
       "25         0.050496   6828710  438993545         134.000   8.811808   \n",
       "26         0.049682    751448  448951587         147.000  10.854042   \n",
       "27         0.053413    907015  457325097         127.003  10.963228   \n",
       "28         0.050199   4264744  474206692         141.783  11.081486   \n",
       "29         0.057764   3702346  482380491         149.000  10.170138   \n",
       "...             ...       ...        ...             ...        ...   \n",
       "24612      0.025870   5407179  293806370         174.855   9.218582   \n",
       "24613      0.000000    136171  318781800         116.000   0.000000   \n",
       "24614      0.042333    875281  331643924           0.000  11.051783   \n",
       "24615      0.024852  11884453  343264984         162.646  12.772590   \n",
       "24616      0.042406  10568980  362999773         160.000  12.081657   \n",
       "24617      0.042350   1342020  389685109         160.247  11.683026   \n",
       "24618      0.044034  13705737  395730773           0.000  12.336463   \n",
       "24619      0.025554   3630588  405364522         150.000   9.681834   \n",
       "24620      0.029815  14204020  412533726         137.940   8.055731   \n",
       "24621      0.040221   7430612  424658051         144.000   6.390744   \n",
       "24622      0.038236  10917968  436534436         141.000  11.553000   \n",
       "24623      0.026247  14066703  440991836         110.000   5.573852   \n",
       "24624      0.033659   3055418  446480438         167.000  12.173566   \n",
       "24625      0.045299   2071522  453823384         138.000   8.263177   \n",
       "24626      0.037828   5407179  461520942         136.746   9.130300   \n",
       "24627      0.033055  10177904  463509097         132.947   7.000633   \n",
       "24628      0.044729   3091798  471653292         149.000  11.752661   \n",
       "24629      0.046218  10028940  483490146         140.000   9.735862   \n",
       "24630      0.033989    431375  495353301         150.000  12.981242   \n",
       "24631      0.033755    818925  501915815         158.807  10.226917   \n",
       "24632      0.045333   3235195  531829036         138.668  11.263830   \n",
       "24633      0.045869  10057948  549001668         136.000   8.965123   \n",
       "24634      0.042105   1058434  586404536         169.000   9.196492   \n",
       "24635      0.032048   6796415  595027908         133.000  12.161567   \n",
       "24636      0.033409   4164701  595733242         141.065   9.925042   \n",
       "24637      0.035668   2692363  619012388         145.000  10.550808   \n",
       "24638      0.045722   6716410  621586730         167.000  10.261841   \n",
       "24639      0.013025    885462  631384733           0.000  12.426982   \n",
       "24640      0.033549   5386323  637964005          97.000   7.560915   \n",
       "24641      0.026687   9336459  638057350         128.533  12.437240   \n",
       "\n",
       "       elapsed_time  \n",
       "0                 0  \n",
       "1           1016534  \n",
       "2          25389273  \n",
       "3          14255297  \n",
       "4          15717111  \n",
       "5           2190253  \n",
       "6          18987867  \n",
       "7         112583114  \n",
       "8          22200235  \n",
       "9          63173301  \n",
       "10                0  \n",
       "11         21332383  \n",
       "12         25786842  \n",
       "13          5467964  \n",
       "14                0  \n",
       "15                0  \n",
       "16         74726938  \n",
       "17          2120257  \n",
       "18         10636094  \n",
       "19        116859704  \n",
       "20         41686324  \n",
       "21         43442345  \n",
       "22         79374806  \n",
       "23         78623135  \n",
       "24        195609592  \n",
       "25         54115776  \n",
       "26        158112304  \n",
       "27        124599456  \n",
       "28         63799984  \n",
       "29        136925610  \n",
       "...             ...  \n",
       "24612      15595756  \n",
       "24613      97714109  \n",
       "24614       7094829  \n",
       "24615      24274934  \n",
       "24616      21943668  \n",
       "24617     138804482  \n",
       "24618      16438472  \n",
       "24619      12861001  \n",
       "24620      19949857  \n",
       "24621      36704012  \n",
       "24622      24609522  \n",
       "24623      17878312  \n",
       "24624      46542590  \n",
       "24625      53224152  \n",
       "24626      45753260  \n",
       "24627      52514019  \n",
       "24628      69203011  \n",
       "24629      45197458  \n",
       "24630     142720359  \n",
       "24631      67384700  \n",
       "24632      83035204  \n",
       "24633      29883418  \n",
       "24634      49414127  \n",
       "24635      50709917  \n",
       "24636      45610756  \n",
       "24637      94402472  \n",
       "24638      44348845  \n",
       "24639      97302610  \n",
       "24640      50130575  \n",
       "24641      56859618  \n",
       "\n",
       "[24642 rows x 10 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 107 ms\n"
     ]
    }
   ],
   "source": [
    "pd.read_csv('f_clusters4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'Can not create a Path from an empty string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5a4bef8769f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f_cluster.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'Can not create a Path from an empty string'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 505 ms\n"
     ]
    }
   ],
   "source": [
    "df_fro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "time: 1.19 ms\n"
     ]
    }
   ],
   "source": [
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|diff_altitude|geo_distance|\n",
      "+-------------+------------+\n",
      "|  0.014819317| 4.512356E-6|\n",
      "| 0.0105275875|2.9843734E-6|\n",
      "|  0.004059953| 2.551286E-6|\n",
      "|  0.005942405|1.9085373E-6|\n",
      "| 0.0028244788|3.0166398E-6|\n",
      "| 0.0020299766|1.8026603E-6|\n",
      "| 0.0014347194|2.0295515E-6|\n",
      "| 0.0027931256|1.8971801E-6|\n",
      "| 0.0025310297|  2.81416E-6|\n",
      "| 0.0022742606|1.3907281E-6|\n",
      "| 6.2578224E-4| 5.473361E-6|\n",
      "|  0.002129186|4.9994883E-6|\n",
      "| 0.0039683753| 2.141207E-6|\n",
      "| 0.0025947068|5.0974104E-6|\n",
      "| 0.0025676913|3.7508262E-6|\n",
      "| 0.0026786532|1.2864327E-6|\n",
      "| 0.0022436583| 2.459735E-6|\n",
      "| 0.0035104856|3.0550423E-6|\n",
      "| 0.0017247169|2.2072018E-6|\n",
      "|  0.004075216|1.9128806E-6|\n",
      "+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "time: 887 ms\n"
     ]
    }
   ],
   "source": [
    "unv_df = route_df.select(['diff_altitude', 'geo_distance'])\n",
    "unv_df = unv_df.withColumn('diff_altitude', unvector(route_df['diff_altitude']))\n",
    "unv_df = unv_df.withColumn('geo_distance', unvector(route_df['geo_distance']))\n",
    "\n",
    "unv_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.7 s\n"
     ]
    }
   ],
   "source": [
    "p_df = unv_df.select(['diff_altitude', 'geo_distance']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_altitude</th>\n",
       "      <th>geo_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.000000</td>\n",
       "      <td>25000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.005473</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.030250</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.004579</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       diff_altitude  geo_distance\n",
       "count   25000.000000  25000.000000\n",
       "mean        0.005473      0.000003\n",
       "std         0.030250      0.000014\n",
       "min         0.000000      0.000000\n",
       "25%         0.000626      0.000002\n",
       "50%         0.002366      0.000002\n",
       "75%         0.004579      0.000003\n",
       "max         1.000000      0.001570"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.8 ms\n"
     ]
    }
   ],
   "source": [
    "p_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_altitude</th>\n",
       "      <th>geo_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014819</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010528</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diff_altitude  geo_distance\n",
       "0       0.014819      0.000005\n",
       "1       0.010528      0.000003\n",
       "2       0.004060      0.000003\n",
       "3       0.005942      0.000002\n",
       "4       0.002824      0.000003\n",
       "5       0.002030      0.000002\n",
       "6       0.001435      0.000002\n",
       "7       0.002793      0.000002\n",
       "8       0.002531      0.000003\n",
       "9       0.002274      0.000001"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.06 ms\n"
     ]
    }
   ],
   "source": [
    "p_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize done in 0:00:00.050535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'workoutid',\n",
       " 'userid',\n",
       " 'start_time',\n",
       " 'start_altitude',\n",
       " 'start_latitude',\n",
       " 'start_longitude',\n",
       " 'series_length',\n",
       " 'series_time_delta',\n",
       " 'series_time_delta_average',\n",
       " 'timezone',\n",
       " 'id',\n",
       " 'altitude_max',\n",
       " 'altitude_min',\n",
       " 'calories',\n",
       " 'distance',\n",
       " 'duration',\n",
       " 'heart_rate_avg',\n",
       " 'heart_rate_max',\n",
       " 'hydration',\n",
       " 'speed_avg',\n",
       " 'speed_max',\n",
       " 'humidity',\n",
       " 'temperature',\n",
       " 'wind_speed',\n",
       " 'elapsed_time',\n",
       " 'diff_altitude',\n",
       " 'geo_distance']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 56.1 ms\n"
     ]
    }
   ],
   "source": [
    "route_cols = ['diff_altitude', 'geo_distance']\n",
    "df = vectorize_columns(sumstats, route_cols)\n",
    "df.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- normalize  diff_altitude -------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can not reduce() empty RDD",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-cb0afad428bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormalize_df4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-64ab07e38761>\u001b[0m in \u001b[0;36mnormalize_df4\u001b[0;34m(df, list_cols, scale_dict, debug)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#scaler is the wrapper instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_col_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mscalerModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a863d3c06d90>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginalMin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginalMax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mmin\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \"\"\"\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not reduce() empty RDD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtreeReduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can not reduce() empty RDD"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "normalize_df4(df, route_cols, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.6 ms\n"
     ]
    }
   ],
   "source": [
    "def normalize_df4(df, list_cols, scale_dict = {}, debug=False):\n",
    "    tmp_col_name = 'temp'\n",
    "    r_dict = {}\n",
    "    index = 0\n",
    "    for col_name in list_cols:\n",
    "        if debug:\n",
    "            print '------- normalize ', col_name,'-------'\n",
    "        a = datetime.now()\n",
    "        scale_value = 1\n",
    "        if col_name in scale_dict:\n",
    "            scale_value = scale_dict[col_name]\n",
    "            print scale_value\n",
    "        #scaler is the wrapper instance\n",
    "        scaler = scaler_wrapper(inputCol=col_name, outputCol=tmp_col_name, s_max = scale_value)\n",
    "        scalerModel = scaler.fit(df)\n",
    "        b = datetime.now()\n",
    "        if debug:\n",
    "            print b-a\n",
    "        df = scalerModel.transform(df).drop(col_name)\\\n",
    "            .withColumnRenamed(tmp_col_name, col_name)\n",
    "        #r_dict[col_name] = scaler\n",
    "        r_dict[index] = scaler\n",
    "        c =datetime.now()\n",
    "        if debug:\n",
    "            print c-b\n",
    "        index+=1\n",
    "    return df, r_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f_cluster_df = f_cluster_df.withColumn('diff_altitude', unvector(f_cluster_df['diff_altitude']))\n",
    "f_cluster_df = f_cluster_df.withColumn('geo_distance', unvector(f_cluster_df['geo_distance']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
