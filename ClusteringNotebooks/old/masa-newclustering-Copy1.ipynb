{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "sc = SparkContext.getOrCreate()\n",
    "#traces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "#for ordered dict for traces implementation\n",
    "from collections import OrderedDict\n",
    "#from pyspark.sql.types import VectorUDT\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "#import sources.endomondolib as endo\n",
    "#import sources.pysparkconvenience as ps\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.sql.types import DoubleType\n",
    "#for timedelta manipulation\n",
    "from math import fabs\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#kmeans\n",
    "#from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from  pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SQLContext\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql.functions import col\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.mpl_style = 'default'\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc =SparkContext()\n",
    "\n",
    "\n",
    "pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\")\n",
    "if not \"pyspark-shell\" in pyspark_submit_args: pyspark_submit_args += \" pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "#%load_ext autotime\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#del min\n",
    "#del max\n",
    "\n",
    "data_users='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_with_hr_users.csv'\n",
    "\n",
    "sqlContext = HiveContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.53 ms\n"
     ]
    }
   ],
   "source": [
    "def fix_df(df):\n",
    "    return df.rdd.toDF(df.schema.names)\n",
    "\n",
    "\n",
    "def change_column_names(df, old_names, new_names):\n",
    "    pass\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.5 ms\n"
     ]
    }
   ],
   "source": [
    "#minMaxScaler wrapper since originalMin/Max is only implemented in 2.0\n",
    "class scaler_wrapper():\n",
    "    mmModel = ''\n",
    "    originalMin = ''\n",
    "    originalMax = ''   \n",
    "    \n",
    "    def __init__(self, inputCol, outputCol, s_min = 0, s_max = 0):\n",
    "        self.mmModel = MinMaxScaler(inputCol=inputCol, outputCol=outputCol)\n",
    "        self.mmModel.setMin(s_min)\n",
    "        self.mmModel.setMax(s_max)\n",
    "        self.in_column = inputCol\n",
    "        \n",
    "    def get_input_col_name(self):\n",
    "        return self.mmModel.getInputCol()\n",
    "\n",
    "    def getMax(self):\n",
    "        return self.mmModel.getMax()\n",
    "        \n",
    "    def getMin(self):\n",
    "        return self.mmModel.getMin()\n",
    "    \n",
    "    def describe(self):\n",
    "        print 'describe'\n",
    "    \n",
    "    def fit(self, df):\n",
    "        col = self.mmModel.getInputCol()\n",
    "        self.originalMin = df.select(col).rdd.flatMap(lambda x: x[0]).min()\n",
    "        self.originalMax = df.select(col).rdd.flatMap(lambda x: x[0]).max()\n",
    "        return self.mmModel.fit(df)\n",
    "    \n",
    "    #denormalize the value\n",
    "    def denormalize(self, value):\n",
    "        v = (value-self.getMin())*\\\n",
    "            (self.originalMax - self.originalMin)*\\\n",
    "            (self.getMax()-self.getMin()) + self.originalMin\n",
    "        if v or v == 0:\n",
    "            return v\n",
    "        else:\n",
    "            return -999\n",
    "        \n",
    "    def denormalize_df(self, df):\n",
    "        col = self.mmModel.getInputCol()\n",
    "        \n",
    "        \n",
    "    def normalize(self, value):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 672 ms\n"
     ]
    }
   ],
   "source": [
    "#calculate sse\n",
    "def compute_sse(a,b):\n",
    "    total = 0\n",
    "    if len(a) != len(b):\n",
    "        print 'bad input'\n",
    "        return 99999\n",
    "    else:\n",
    "        for i in range(len(a)):\n",
    "            total += math.pow(float(a[i])-float(b[i]),2)\n",
    "    return math.sqrt(total)\n",
    "\n",
    "def change_type_col_double(df, col):\n",
    "    df = df.withColumn(col, df[col].cast(\"double\"))\n",
    "    return df.na.fill(0)\n",
    "\n",
    "#change column type\n",
    "def change_type_cols_double(df, list_cols):\n",
    "    for col_name in list_cols:\n",
    "        df = df.withColumn(col_name, df[col_name].cast(\"double\"))\n",
    "    return df.na.fill(0)\n",
    "\n",
    "def normalize_vectorize_df(df, list_cols, scale_dict={}):\n",
    "    n_df = vectorize_columns(df, list_cols)\n",
    "    n_df, scalerModels = normalize_df(n_df, list_cols, scale_dict)\n",
    "    return n_df, scalerModels\n",
    "#vectorize the column\n",
    "#keeps the original name\n",
    "def vectorize_columns(df, list_cols, debug=False):\n",
    "    a = datetime.now()\n",
    "    tmp_col_name = 'temp'\n",
    "    for col_name in list_cols:\n",
    "        vectorize = udf(lambda vs: Vectors.dense(vs), VectorUDT())\n",
    "        df = df.withColumn(tmp_col_name, vectorize(df[col_name])).drop(col_name)\\\n",
    "            .withColumnRenamed(tmp_col_name, col_name)\n",
    "    b = datetime.now()\n",
    "    print 'vectorize done in', b-a\n",
    "    return df\n",
    "\n",
    "##scaler wrapper usage\n",
    "#generate normalized dataframe\n",
    "#keep the original column names\n",
    "#returns:\n",
    "#   (df, {col_name:scalerModel, col_name:scalerModel,....})\n",
    "def normalize_df_old(df, list_cols, scale_dict = {}, debug=False):\n",
    "    tmp_col_name = 'temp'\n",
    "    r_dict = {}\n",
    "    index = 0\n",
    "    for col_name in list_cols:\n",
    "        if debug:\n",
    "            print '------- normalize ', col_name,'-------'\n",
    "        a = datetime.now()\n",
    "        scale_value = 1\n",
    "        if col_name in scale_dict:\n",
    "            scale_value = scale_dict[col_name]\n",
    "            print scale_value\n",
    "        #scaler is the wrapper instance\n",
    "        scaler = scaler_wrapper(inputCol=col_name, outputCol=tmp_col_name, s_max = scale_value)\n",
    "        scalerModel = scaler.fit(df)\n",
    "        b = datetime.now()\n",
    "        if debug:\n",
    "            print b-a\n",
    "        df = scalerModel.transform(df).drop(col_name)\\\n",
    "            .withColumnRenamed(tmp_col_name, col_name)\n",
    "        #r_dict[col_name] = scaler\n",
    "        r_dict[index] = scaler\n",
    "        c =datetime.now()\n",
    "        if debug:\n",
    "            print c-b\n",
    "        index+=1\n",
    "    return df, r_dict\n",
    "\n",
    "\n",
    "#   (df, {col_name:scalerModel, col_name:scalerModel,....})\n",
    "def normalize_df(df, list_cols, scale_dict = {}, debug=False):\n",
    "    tmp_col_name = 'temp'\n",
    "    r_dict = {}\n",
    "    index = 0\n",
    "    for col_name in list_cols:\n",
    "        if debug:\n",
    "            print '------- normalize ', col_name,'-------'\n",
    "        a = datetime.now()\n",
    "        scale_value = 1\n",
    "        if col_name in scale_dict:\n",
    "            scale_value = scale_dict[col_name]\n",
    "            print scale_value\n",
    "        #scaler is the wrapper instance\n",
    "        scaler = scaler_wrapper(inputCol=col_name, outputCol=tmp_col_name, s_max = scale_value)\n",
    "        scalerModel = scaler.fit(df)\n",
    "        b = datetime.now()\n",
    "        if debug:\n",
    "            print b-a\n",
    "        df = scalerModel.transform(df).drop(col_name)\\\n",
    "            .withColumnRenamed(tmp_col_name, col_name)\n",
    "        #r_dict[col_name] = scaler\n",
    "        r_dict[index] = scaler\n",
    "        c =datetime.now()\n",
    "        if debug:\n",
    "            print c-b\n",
    "        index+=1\n",
    "    return df, r_dict\n",
    "\n",
    "\n",
    "# MinMaxScaler doesn't have originalMin(only supported in 2.0). made one above cell with wrapper class\n",
    "# #generate normalized dataframe\n",
    "# #keep the original column names\n",
    "# #returns:\n",
    "# #   (df, {col_name:scalerModel, col_name:scalerModel,....})\n",
    "# def normalize_df(df, list_cols):\n",
    "#     tmp_col_name = 'temp'\n",
    "#     r_dict = {}\n",
    "#     for col_name in list_cols:\n",
    "#         scaler = MinMaxScaler(inputCol=col_name, outputCol=tmp_col_name)\n",
    "#         scalerModel = scaler.fit(df)\n",
    "#         df = scalerModel.transform(df).drop(col_name)\\\n",
    "#             .withColumnRenamed(tmp_col_name, col_name)\n",
    "#         r_dict[col_name] = [scaler, scalerModel]\n",
    "#     return df, r_dict\n",
    "\n",
    "#input: (MLLIB KMEANS)\n",
    "#   cols_to_cluster: list of column names to cluster\n",
    "# def cluster_summary_df(df, cols_to_cluster, num_clusters = 2):\n",
    "#     vecAssembler = VectorAssembler(inputCols=cols_to_cluster,\\\n",
    "#         outputCol='features')\n",
    "#     df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "#     df_to_cluster = df_to_cluster.select('features')\n",
    "#     df_to_cluster = df_to_cluster.rdd\\\n",
    "#         .map(lambda row : Vectors.dense([item for item in row]))\n",
    "#     clusters = KMeans.train(df_to_cluster, num_clusters,\\\n",
    "#                            maxIterations = 10,\\\n",
    "#                         initializationMode=\"random\")\n",
    "    \n",
    "#     return clusters\n",
    "\n",
    "\n",
    "#create feature column from list of cols\n",
    "def create_feature_column(df, cols_to_cluster):\n",
    "    #make dup columns of the ones that will be vectorized\n",
    "    normalized_names = [i+'_n' for i in cols_to_cluster]\n",
    "    \n",
    "    norm_and_woutid = [i+'_n' for i in cols_to_cluster]\n",
    "    norm_and_woutid.append('workoutid')\n",
    "\n",
    "    normalized_df = df\n",
    "    \n",
    "    for c in range(len(cols_to_cluster)):\n",
    "        normalized_df = normalized_df.withColumnRenamed(\\\n",
    "                        cols_to_cluster[c], normalized_names[c])\n",
    "    normalized_df = normalized_df.select(norm_and_woutid)\n",
    "\n",
    "    \n",
    "    df = df.join(normalized_df, (df['workoutid']==normalized_df['workoutid']),\\\n",
    "                'inner').drop(normalized_df['workoutid'])\n",
    "    #df = df.crossJoin(normalized_df)\n",
    "    vecAssembler = VectorAssembler(inputCols=normalized_names,\\\n",
    "        outputCol='features')\n",
    "\n",
    "    df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "\n",
    "    #normed_df = df_to_cluster.select(normalized_names)\n",
    "    return df_to_cluster   \n",
    "    \n",
    "    \n",
    "#ML KMEANS\n",
    "#returns KMeanModel, df_to_cluster\n",
    "def cluster_summary_df(df, cols_to_cluster, num_clusters = 2, init_var=True, bisectingKMean = True,\\\n",
    "                      debug = False):\n",
    "    a = datetime.now()\n",
    "\n",
    "#     #make dup columns of the ones that will be vectorized\n",
    "#     normalized_names = [i+'_n' for i in cols_to_cluster]\n",
    "    \n",
    "#     norm_and_woutid = [i+'_n' for i in cols_to_cluster]\n",
    "#     norm_and_woutid.append('workoutid')\n",
    "#     #make copy\n",
    "#     a = datetime.now()\n",
    "#     normalized_df = df\n",
    "    \n",
    "#     b = datetime.now()\n",
    "#     print b-a , 'df copy'\n",
    "    \n",
    "#     for c in range(len(cols_to_cluster)):\n",
    "#         normalized_df = normalized_df.withColumnRenamed(\\\n",
    "#                         cols_to_cluster[c], normalized_names[c])\n",
    "#     normalized_df = normalized_df.select(norm_and_woutid)\n",
    "\n",
    "    \n",
    "#     df = df.join(normalized_df, (df['workoutid']==normalized_df['workoutid']),\\\n",
    "#                 'inner').drop(normalized_df['workoutid'])\n",
    "#     #df = df.crossJoin(normalized_df)\n",
    "#     vecAssembler = VectorAssembler(inputCols=normalized_names,\\\n",
    "#         outputCol='features')\n",
    "\n",
    "#     df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "\n",
    "#     normed_df = df_to_cluster.select(normalized_names)\n",
    "    \n",
    "    \n",
    "#    df_to_cluster = df_to_cluster.select(['workoutid','features'])\n",
    "    #df_to_cluster = df_to_cluster.rdd\\\n",
    "     #   .map(lambda row : Vectors.dense([item for item in row]))\n",
    "    df_to_cluster = create_feature_column(df, cols_to_cluster)\n",
    "    \n",
    "        \n",
    "        \n",
    "    if not bisectingKMean:\n",
    "        if not init_var:\n",
    "            kmeans = KMeans(k=num_clusters, seed=1, initMode=\"random\",\\\n",
    "                           featuresCol=cols_to_cluster)\n",
    "        else:\n",
    "            kmeans = KMeans(k=num_clusters, seed=1, initMode=\"k-means||\")\n",
    "    else:\n",
    "\n",
    "        kmeans = BisectingKMeans().setK(num_clusters).setSeed(1).setMinDivisibleClusterSize(10)\n",
    "    \n",
    "    \n",
    "    model = kmeans.fit(df_to_cluster)\n",
    "    \n",
    "    b = datetime.now()\n",
    "    if debug:\n",
    "        print 'clustering in ', b-a\n",
    "    return model, df_to_cluster\n",
    "\n",
    "\n",
    "\n",
    "# def cluster_summary_df(df, cols_to_cluster, num_clusters = 2, init_var=False):\n",
    "#     #make dup columns of the ones that will be vectorized\n",
    "#     normalized_names = [i+'_n' for i in cols_to_cluster]\n",
    "    \n",
    "#     norm_and_woutid = [i+'_n' for i in cols_to_cluster]\n",
    "#     norm_and_woutid.append('workoutid')\n",
    "#     #make copy\n",
    "#     a = datetime.now()\n",
    "#     normalized_df = df.copy()\n",
    "    \n",
    "#     b = datetime.now()\n",
    "#     print b-a , 'df copy'\n",
    "    \n",
    "#     for c in range(len(cols_to_cluster)):\n",
    "#         normalized_df = normalized_df.withColumnRenamed(\\\n",
    "#                         cols_to_cluster[c], normalized_names[c])\n",
    "#     normalized_df = normalized_df.select(norm_and_woutid)\n",
    "\n",
    "    \n",
    "#     df = df.join(normalized_df, (df['workoutid']==normalized_df['workoutid']),\\\n",
    "#                 'inner').drop(normalized_df['workoutid'])\n",
    "#     #df = df.crossJoin(normalized_df)\n",
    "#     vecAssembler = VectorAssembler(inputCols=normalized_names,\\\n",
    "#         outputCol='features')\n",
    "\n",
    "#     df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "\n",
    "#     normed_df = df_to_cluster.select(normalized_names)\n",
    "    \n",
    "    \n",
    "# #    df_to_cluster = df_to_cluster.select(['workoutid','features'])\n",
    "#     #df_to_cluster = df_to_cluster.rdd\\\n",
    "#      #   .map(lambda row : Vectors.dense([item for item in row]))\n",
    "        \n",
    "#     print 'clustering times'\n",
    "#     a = datetime.now()\n",
    "#     if not init_var:\n",
    "#         kmeans = KMeans(k=num_clusters, seed=1, initMode=\"random\",\\\n",
    "#                        featuresCol=cols_to_cluster)\n",
    "#     else:\n",
    "#         kmeans = KMeans(k=num_clusters, seed=1, initMode=\"k-means||\")\n",
    "#     b = datetime.now()\n",
    "#     print b - a\n",
    "    \n",
    "#     model = kmeans.fit(df_to_cluster)\n",
    "    \n",
    "#     c = datetime.now()\n",
    "#     print c - b\n",
    "#     return model, df_to_cluster\n",
    "\n",
    "    \n",
    "#returns clusters. show cluster centerse by clusters.clusterCenters.\n",
    "#output:\n",
    "#   [0]: KMeanModel\n",
    "#   [1]: DF that was clustered (normalized)\n",
    "#   [2]: ScalerModels\n",
    "#   [3]: error\n",
    "def cluster_df(df, cols_to_normalize, num_clusters = 2, return_sse = False, cols_to_cluster = [], init_var=True,\\\n",
    "              scale_dict = {}, bisectingKMean = True, debug = False):\n",
    "    if not cols_to_cluster:\n",
    "        cols_to_cluster = cols_to_normalize\n",
    "    df = change_type_cols_double(df, cols_to_normalize)\n",
    "    df = vectorize_columns(df, cols_to_normalize, debug)\n",
    "    df, scalerModels = normalize_df(df, cols_to_cluster, scale_dict, debug)\n",
    "    r_obj = cluster_summary_df(df, cols_to_cluster, num_clusters, init_var, bisectingKMean, \\\n",
    "                              debug)\n",
    "    sse = 'none'\n",
    "    if return_sse:\n",
    "        #calculate SSE for this kmean model\n",
    "        kmodel = r_obj[0]\n",
    "        df_to_cluster = r_obj[1]\n",
    "        centers = kmodel.clusterCenters()\n",
    "        prediction_df = kmodel.transform(df_to_cluster)\n",
    "\n",
    "        #set up rdd\n",
    "        rdd = prediction_df.rdd.map(lambda (a,b):\\\n",
    "                                   (a, centers[b]))\n",
    "        sse = rdd.map(lambda (a,b): compute_sse(a,b)).sum()\n",
    "    \n",
    "    return r_obj[0], r_obj[1], scalerModels, sse\n",
    "\n",
    "def denormalize_centers(list_centers, scaler_models):\n",
    "    new_centers = []\n",
    "    for center in list_centers:\n",
    "        n_center = []\n",
    "        for c in range(len(center)):\n",
    "            n_center.append(scaler_models[c].denormalize(center[c]))\n",
    "        new_centers.append(n_center)\n",
    "    return new_centers\n",
    "\n",
    "\n",
    "\n",
    "def denormalize_cluster(cluster_centers, list_scaler_models):\n",
    "    r_centers = []\n",
    "    for center in cluster_centers:\n",
    "        r_center = []\n",
    "        for index in range(len(center)):\n",
    "            s_model = list_scaler_models[index]\n",
    "            r_center.append(s_model.denormalize(center[index]))\n",
    "        r_centers.append(r_center)\n",
    "    return r_centers\n",
    "\n",
    "def get_distribution_of_workouts(transformed_df):\n",
    "    #count of predictions\n",
    "    return transformed_df.groupBy('prediction').count().orderBy('prediction')\n",
    "\n",
    "#transform (add prediction column) to the dataframe based on the kmeamodel input\n",
    "def transform_df_with_kmeamodel(kmeanModel, df, cols_to_cluster, scale_dict={}):\n",
    "    #need to vectorize and normalize the df\n",
    "    df_to_transform, scalerModels = normalize_vectorize_df(big_sumstats, cols_to_cluster, scale_dict)\n",
    "    df_to_transform = create_feature_column(df_to_transform, cols_to_cluster)\n",
    "    return kmeanModel.transform(df_to_transform), scalerModels\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "def vectorizeData(data):\n",
    "    return data.rdd.map(lambda r: [r[0], r[1], r[2], Vectors.dense(r[3:6]), r[7]]).toDF([\\\n",
    "            'prediction', 'userid', 'workoutid', 'label', 'max_elapsed_time'])\n",
    "\n",
    "# total_df= fix_df(sumstats.join(prediction_df.select('workoutid','prediction'), \\\n",
    "#                                sumstats['workoutid']==prediction_df['workoutid'],'inner').\\\n",
    "#                             drop(prediction_df['workoutid']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[altitude: decimal(20,10), heart_rate: decimal(10,5), latitude: decimal(20,10), longitude: decimal(20,10), speed: decimal(20,10), workoutid: int, time: int, altitude_first: decimal(10,5), altitude_second: decimal(10,5), elapsed_time: int, speed_first: decimal(10,5), geo_distance: decimal(20,10), elapsed_distance: decimal(15,10), heart_rate_ma_25: decimal(8,5), speed_ma_100: decimal(8,5), speed_ma_50: decimal(8,5), speed_by_geo: decimal(20,10)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrameReader\n",
    "\n",
    "url = 'postgresql://172.31.29.1:5432/endomondo_sample'\n",
    "#url = 'postgresql://172.31.29.1:5432/endomondo'\n",
    "\n",
    "properties = {'user':'endomondo', 'password': 'End0m0ndo'}\n",
    "\n",
    "df_runs = DataFrameReader(sqlContext).jdbc(url='jdbc:%s' % url, table='run', properties=properties)\n",
    "\n",
    "df_users = DataFrameReader(sqlContext).jdbc(url='jdbc:%s' % url, table='run_by_workout', properties=properties)\n",
    "\n",
    "df_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "sub_df = df_runs.limit(1000)\n",
    "df_users = df_users.withColumn('diff_altitude', \\\n",
    "                    df_users['altitude_max'] \\\n",
    "                    -df_users['altitude_min'])\n",
    "sumstats = df_users.limit(100)#.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 897 µs\n"
     ]
    }
   ],
   "source": [
    "route_cols = ['diff_altitude', 'geo_distance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize done in 0:00:00.129901\n",
      "------- normalize  diff_altitude -------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-422969ea0efc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscale_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mr_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0mscale_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbisectingKMean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'done'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0932dcfc422e>\u001b[0m in \u001b[0;36mcluster_df\u001b[0;34m(df, cols_to_normalize, num_clusters, return_sse, cols_to_cluster, init_var, scale_dict, bisectingKMean, debug)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_type_cols_double\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_to_normalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_to_normalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalerModels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_to_cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0mr_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_summary_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_to_cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbisectingKMean\u001b[0m\u001b[0;34m,\u001b[0m                               \u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0msse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0932dcfc422e>\u001b[0m in \u001b[0;36mnormalize_df\u001b[0;34m(df, list_cols, scale_dict, debug)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m#scaler is the wrapper instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_col_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mscalerModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a863d3c06d90>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginalMin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginalMax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mmin\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \"\"\"\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "scale_dict = {}\n",
    "\n",
    "r_obj = cluster_df(sumstats, route_cols, 5, init_var=True,\\\n",
    "                  scale_dict=scale_dict, bisectingKMean=True,\\\n",
    "                  debug=True)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.02 ms\n"
     ]
    }
   ],
   "source": [
    "route_kmodel = r_obj[0]\n",
    "route_df = r_obj[1]\n",
    "route_scalerModels = r_obj[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 711 µs\n"
     ]
    }
   ],
   "source": [
    "###check if routes make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 311 ms\n"
     ]
    }
   ],
   "source": [
    "route_prediction_df = route_kmodel.transform(route_df)\n",
    "route_prediction_cols = ['route_prediction','route_features']\n",
    "distinct_clusters = route_prediction_df.select('prediction').distinct().collect()\n",
    "route_prediction_df = route_prediction_df.withColumnRenamed('prediction', 'route_prediction')\n",
    "route_prediction_df = route_prediction_df.withColumnRenamed('features', 'route_features')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|route_prediction|count|\n",
      "+----------------+-----+\n",
      "|               2|  152|\n",
      "|               1|  327|\n",
      "|               0|  358|\n",
      "|               3|  151|\n",
      "|               4|   12|\n",
      "+----------------+-----+\n",
      "\n",
      "time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "route_prediction_df.groupBy('route_prediction').count().show()\n",
    "route_prediction_df = fix_df(route_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.00561581,  0.08662023]),\n",
       " array([ 0.00825486,  0.14370067]),\n",
       " array([ 0.01178036,  0.19603238]),\n",
       " array([ 0.01898868,  0.28943591]),\n",
       " array([ 0.03199172,  0.68608366])]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.82 ms\n"
     ]
    }
   ],
   "source": [
    "route_kmodel.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[34.337319662011183, 1601.0664774709555],\n",
       " [50.473503174311958, 2656.1269716579118],\n",
       " [72.0298458157895, 3623.4131298017123],\n",
       " [116.1043967549669, 5349.8604437707518],\n",
       " [195.610175, 12681.397421410449]]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.86 ms\n"
     ]
    }
   ],
   "source": [
    "denormalize_cluster(route_kmodel.clusterCenters(),  route_scalerModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|  358|\n",
      "|         1|  327|\n",
      "|         2|  152|\n",
      "|         3|  151|\n",
      "|         4|   12|\n",
      "+----------+-----+\n",
      "\n",
      "time: 313 ms\n"
     ]
    }
   ],
   "source": [
    "distribution_df = get_distribution_of_workouts(route_kmodel.transform(route_df))\n",
    "distribution_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 712 µs\n"
     ]
    }
   ],
   "source": [
    "#######################end route checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 0, 3, 4]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.4 ms\n"
     ]
    }
   ],
   "source": [
    "#get cluster numbers\n",
    "cluster_numbers = [int(distinct_clusters[i][0]) for i in range(len(distinct_clusters))]\n",
    "cluster_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.5 ms\n"
     ]
    }
   ],
   "source": [
    "#create list of dataframes for each route_prediction value\n",
    "list_route_clusters = [route_prediction_df.where(route_prediction_df['route_prediction']==i) \\\n",
    "                      for i in cluster_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vectorize done in 0:00:00.067778\n",
      "transforming\n",
      "\n",
      "\n",
      "joined 0:00:00.055359\n",
      "1 vectorize done in 0:00:00.072193\n",
      "transforming\n",
      "\n",
      "\n",
      "joined 0:00:00.056016\n",
      "2 vectorize done in 0:00:00.073228\n",
      "transforming\n",
      "\n",
      "\n",
      "joined 0:00:00.061502\n",
      "3 vectorize done in 0:00:00.072374\n",
      "transforming\n",
      "\n",
      "\n",
      "joined 0:00:00.055361\n",
      "4 vectorize done in 0:00:00.068054\n",
      "transforming\n",
      "\n",
      "\n",
      "joined 0:00:00.055101\n",
      "time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "#got list of dataframes by route cluster. for each of them, create new clusters \n",
    "#add a new column 'performance_prediction' for each cluster\n",
    "\n",
    "performance_cols_to_cluster = ['heart_rate_avg', 'speed_avg', 'elapsed_time']\n",
    "\n",
    "dict_cluster_to_objs = {}\n",
    "\n",
    "n_list_route_clusters = []\n",
    "\n",
    "for i in range(len(list_route_clusters)):\n",
    "    print i, \n",
    "#     #cluster\n",
    "#     c_df = list_route_clusters[i]\n",
    "#     r_obj = cluster_df(c_df, performance_cols_to_cluster, 5, init_var=True, \\\n",
    "#                       scale_dict = scale_dict, bisectingKMean=True,\\\n",
    "#                       debug=False)\n",
    "#     dict_cluster_to_objs[i] = r_obj\n",
    "\n",
    "#     #transform the dataset\n",
    "#     p_kmodel = r_obj[0]\n",
    "#     p_df_to_transform = r_obj[1]\n",
    "#     p_scalerModels = r_obj[2]\n",
    "    \n",
    "#     p_kmodel.transform(p_df_to_transform)\n",
    "    c_df = list_route_clusters[i]\n",
    "    #cluster within the route cluster dataframe\n",
    "    r_obj = cluster_df(c_df, performance_cols_to_cluster, 3, init_var=True, \\\n",
    "                      scale_dict = scale_dict, bisectingKMean=True,\\\n",
    "                      debug=False)\n",
    "    dict_cluster_to_objs[i] = r_obj\n",
    "\n",
    "    #transform the dataset\n",
    "    p_kmodel = r_obj[0]\n",
    "    p_df_to_transform = r_obj[1]\n",
    "    p_scalerModels = r_obj[2]\n",
    "    print 'transforming'\n",
    "    temp_df = p_kmodel.transform(p_df_to_transform)\n",
    "    temp_df = temp_df.withColumnRenamed('features', 'perf_features')\n",
    "    temp_df = temp_df.withColumnRenamed('prediction', 'perf_prediction')\n",
    "    #print c_df.schema\n",
    "    print '\\n'\n",
    "    #transform\n",
    "    c_df = c_df.drop('perf_features')\n",
    "    c_df = c_df.drop('perf_prediction')\n",
    "    a = datetime.now()\n",
    "\n",
    "    #list_route_clusters[i] = c_df.join(temp_df.select('workoutid','perf_prediction', 'perf_features'),\\\n",
    "    #                                  temp_df['workoutid']==c_df['workoutid'], 'inner')\\\n",
    "    #                                    .drop(temp_df['workoutid'])\n",
    "    n_list_route_clusters.append(c_df.join(temp_df.select('workoutid','perf_prediction', 'perf_features'),\\\n",
    "                                      temp_df['workoutid']==c_df['workoutid'], 'inner')\\\n",
    "                                        .drop(temp_df['workoutid']))\n",
    "    b = datetime.now()\n",
    "    print 'joined', b-a\n",
    "    #list_route_clusters[i].show()\n",
    "    #break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.17 s\n"
     ]
    }
   ],
   "source": [
    "prediction_columns = ['route_prediction', 'perf_prediction']\n",
    "frames = [df.toPandas() for df in n_list_route_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 408 ms\n"
     ]
    }
   ],
   "source": [
    "new_concat_df = sqlContext.createDataFrame(pd.concat(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+-----+\n",
      "|route_prediction|perf_prediction|count|\n",
      "+----------------+---------------+-----+\n",
      "|               0|              0|   32|\n",
      "|               0|              1|  229|\n",
      "|               0|              2|   97|\n",
      "|               1|              0|   44|\n",
      "|               1|              1|  181|\n",
      "|               1|              2|  102|\n",
      "|               2|              0|   13|\n",
      "|               2|              1|   98|\n",
      "|               2|              2|   41|\n",
      "|               3|              0|   11|\n",
      "|               3|              1|   82|\n",
      "|               3|              2|   58|\n",
      "|               4|              0|    8|\n",
      "|               4|              1|    4|\n",
      "+----------------+---------------+-----+\n",
      "\n",
      "time: 315 ms\n"
     ]
    }
   ],
   "source": [
    "new_concat_df.groupBy(['route_prediction','perf_prediction']).count()\\\n",
    ".orderBy(['route_prediction', 'perf_prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.1 ms\n"
     ]
    }
   ],
   "source": [
    "#remove small clusters, might be outliers\n",
    "new_concat_df = new_concat_df.where(\n",
    "    ((new_concat_df['route_prediction']!= 3) | (new_concat_df['perf_prediction']!= 0))&\\\n",
    "    ((new_concat_df['route_prediction']!= 4) | (new_concat_df['perf_prediction']!= 0))&\\\n",
    "    ((new_concat_df['route_prediction']!= 4) | (new_concat_df['perf_prediction']!= 1))\\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['route_prediction', 'perf_prediction', 'diff_altitude', 'geo_distance', 'userid', 'workoutid', 'heart_rate_avg', 'speed_avg', 'elapsed_time']\n",
      "time: 1.47 ms\n"
     ]
    }
   ],
   "source": [
    "select_cols = prediction_columns+route_cols+['userid','workoutid']+performance_cols_to_cluster\n",
    "print select_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.09 ms\n"
     ]
    }
   ],
   "source": [
    "sum_cols = route_cols+performance_cols_to_cluster+['workoutid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.09 ms\n"
     ]
    }
   ],
   "source": [
    "#magic\n",
    "unvector = udf(lambda value: value[0].item(), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.5 ms\n"
     ]
    }
   ],
   "source": [
    "f_cluster_df = new_concat_df.select(select_cols)\n",
    "f_cluster_df = f_cluster_df.withColumn('diff_altitude', unvector(f_cluster_df['diff_altitude']))\n",
    "f_cluster_df = f_cluster_df.withColumn('geo_distance', unvector(f_cluster_df['geo_distance']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 910 ms\n"
     ]
    }
   ],
   "source": [
    "f_cluster_df.write.csv('f_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "time: 1.19 ms\n"
     ]
    }
   ],
   "source": [
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
