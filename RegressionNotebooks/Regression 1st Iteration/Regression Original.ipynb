{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "#import sources.endomondolib as endo\n",
    "#import sources.pysparkconvenience as ps\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import SQLContext\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql.functions import col, mean, min, max\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, GBTRegressor, RandomForestRegressor\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.mpl_style = 'default'\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, 172.31.42.143, executor 0): java.io.FileNotFoundException: File file:/home/ubuntu/dave2.csv/part-00131-43e007d8-f04d-48f3-8777-8390676cb3c3.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:252)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:251)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:102)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: File file:/home/ubuntu/dave2.csv/part-00131-43e007d8-f04d-48f3-8777-8390676cb3c3.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:252)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:251)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:102)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-25407c08534d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dave2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#hereâ€™s the new vectorizer function:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-25407c08534d>\u001b[0m in \u001b[0;36mdf_from_csv\u001b[0;34m(csv_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0;31m#didnâ€™t work with take(1). believe returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0;31m#different object then first()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, 172.31.42.143, executor 0): java.io.FileNotFoundException: File file:/home/ubuntu/dave2.csv/part-00131-43e007d8-f04d-48f3-8777-8390676cb3c3.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:252)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:251)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:102)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: File file:/home/ubuntu/dave2.csv/part-00131-43e007d8-f04d-48f3-8777-8390676cb3c3.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:252)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:251)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:102)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 544 ms\n"
     ]
    }
   ],
   "source": [
    "#sc = SQLContext()\n",
    "#create pyspark dataframe from csv\n",
    "def df_from_csv(csv_file):\n",
    "    text = sc.textFile(csv_file)\\\n",
    "       .map(lambda line: line.split(','))\n",
    "   #didnâ€™t work with take(1). believe returns\n",
    "   #different object then first()\n",
    "    schema = text.first()\n",
    "    data = text.filter(lambda x: x != schema)\n",
    "    df = sqlContext.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "df = df_from_csv('dave2.csv')\n",
    "\n",
    "#hereâ€™s the new vectorizer function:\n",
    "\n",
    "def vectorizeData(data):\n",
    "    return data.rdd.map(lambda r: [r[0], r[1], r[2], Vectors.dense(r[3:6]), float(r[7])]).toDF([\\\n",
    "           'cluster', 'userid', 'workoutid', 'features', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2633d48b0a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mselect_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'userid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'workoutid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum_geo_distance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'diff_altitude'\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;34m'new_avg_speed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'new_avg_dist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_elapsed_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselect_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.4 ms\n"
     ]
    }
   ],
   "source": [
    "select_columns = ['prediction', 'userid', 'workoutid', 'sum_geo_distance', 'diff_altitude', \\\n",
    "                'new_avg_speed', 'new_avg_dist', 'max_elapsed_time']\n",
    "df = df.select(select_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b56e2f3c56e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.2 ms\n"
     ]
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "reg_df = vectorizeData(df)\n",
    "\n",
    "distinct_clusters = reg_df.select('cluster').distinct().collect()\n",
    "cluster_numbers = [int(distinct_clusters[i][0]) for i in range(len(distinct_clusters))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(cluster,StringType,true),StructField(userid,StringType,true),StructField(workoutid,StringType,true),StructField(features,VectorUDT,true),StructField(label,DoubleType,true)))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.29 ms\n"
     ]
    }
   ],
   "source": [
    "reg_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reg_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-28b9afeca950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mreg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reg_df' is not defined"
     ]
    }
   ],
   "source": [
    "print reg_df[reg_df['cluster']==i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(cluster=u'7')\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o157.equalTo.\n: java.lang.RuntimeException: Unsupported literal type class java.util.ArrayList [7]\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:75)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:101)\n\tat org.apache.spark.sql.Column.$eq$eq$eq(Column.scala:267)\n\tat org.apache.spark.sql.Column.equalTo(Column.scala:290)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-db3a3d926039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistinct_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mreg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/column.pyc\u001b[0m in \u001b[0;36m_\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mnjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o157.equalTo.\n: java.lang.RuntimeException: Unsupported literal type class java.util.ArrayList [7]\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:75)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:101)\n\tat org.apache.spark.sql.Column.$eq$eq$eq(Column.scala:267)\n\tat org.apache.spark.sql.Column.equalTo(Column.scala:290)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 67.5 ms\n"
     ]
    }
   ],
   "source": [
    "#broken\n",
    "for i in distinct_clusters:\n",
    "    print i\n",
    "    print reg_df[reg_df['cluster']==i].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3, 8, 0, 5, 6, 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.64 ms\n"
     ]
    }
   ],
   "source": [
    "cluster_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 7\n",
      "Root Mean Squared Error (RMSE) on test data = 277.277\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 3\n",
      "Root Mean Squared Error (RMSE) on test data = 40.1564\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 8\n",
      "Root Mean Squared Error (RMSE) on test data = 638.485\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 0\n",
      "Root Mean Squared Error (RMSE) on test data = 562.182\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 5\n",
      "Root Mean Squared Error (RMSE) on test data = 823.759\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 6\n",
      "Root Mean Squared Error (RMSE) on test data = 197.481\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 2\n",
      "Root Mean Squared Error (RMSE) on test data = 444.264\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 7\n",
      "Root Mean Squared Error (RMSE) on test data = 0.99637\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 3\n",
      "Root Mean Squared Error (RMSE) on test data = 0\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 8\n",
      "Root Mean Squared Error (RMSE) on test data = 0\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 0\n",
      "Root Mean Squared Error (RMSE) on test data = 90.6457\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 5\n",
      "Root Mean Squared Error (RMSE) on test data = 252.745\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 6\n",
      "Root Mean Squared Error (RMSE) on test data = 0\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 2\n",
      "Root Mean Squared Error (RMSE) on test data = 263.962\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 7\n",
      "Root Mean Squared Error (RMSE) on test data = 0.014673\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 3\n",
      "Root Mean Squared Error (RMSE) on test data = 0\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 8\n",
      "Root Mean Squared Error (RMSE) on test data = 0\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 0\n",
      "Root Mean Squared Error (RMSE) on test data = 4.03827\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 5\n",
      "Root Mean Squared Error (RMSE) on test data = 21.1848\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 6\n",
      "Root Mean Squared Error (RMSE) on test data = 0\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 2\n",
      "Root Mean Squared Error (RMSE) on test data = 60.4727\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 7\n",
      "Root Mean Squared Error (RMSE) on test data = 143.955\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 3\n",
      "Root Mean Squared Error (RMSE) on test data = 60.6928\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 8\n",
      "Root Mean Squared Error (RMSE) on test data = 470.93\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 0\n",
      "Root Mean Squared Error (RMSE) on test data = 261.819\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 5\n",
      "Root Mean Squared Error (RMSE) on test data = 447.991\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 6\n",
      "Root Mean Squared Error (RMSE) on test data = 94.9385\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 2\n",
      "Root Mean Squared Error (RMSE) on test data = 283.978\n",
      "time: 6min 19s\n"
     ]
    }
   ],
   "source": [
    "#masa edit\n",
    "model_list = [LinearRegression(featuresCol=\"features\", labelCol=\"label\"),\\\n",
    "              DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\"), \\\n",
    "              GBTRegressor(featuresCol=\"features\", labelCol=\"label\"),\\\n",
    "              RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")]\n",
    "\n",
    "\n",
    "distinct_clusters = reg_df.select('cluster').distinct().collect()\n",
    "\n",
    "cluster_numbers = [int(distinct_clusters[i][0]) for i in range(len(distinct_clusters))]\n",
    "\n",
    "for n in model_list:    \n",
    "    for i in cluster_numbers:\n",
    "        temp_df = reg_df[reg_df['cluster']==i]\n",
    "        temp_lr = n\n",
    "        temp_lrModel = temp_lr.fit(temp_df['label','features'])\n",
    "        temp_df = temp_lrModel.transform(temp_df)\n",
    "        \n",
    "        #paramGrid = ParamGridBuilder() \\\n",
    "        #.addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "        #.addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "        #.build()\n",
    "\n",
    "        \n",
    "        \n",
    "        evaluator = RegressionEvaluator(\n",
    "            labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        \n",
    "        #crossval = CrossValidator(estimator=pipeline,\n",
    "        #                          estimatorParamMaps=paramGrid,\n",
    "        #                          evaluator=BinaryClassificationEvaluator(),\n",
    "        #                          numFolds=3)  # use 3+ folds in practice#\n",
    "\n",
    "        # Run cross-validation, and choose the best set of parameters.\n",
    "        #cvModel = crossval.fit(training)\n",
    "\n",
    "        #pred = cvModel.transform(temp_df)\n",
    "        \n",
    "        print(\"For \" + str(type(temp_lr)) + \" and cluster \" + str(i))\n",
    "        #Print the coefficients and intercept for linear regression\n",
    "        #print(\"Coefficients: \" + str(temp_lrModel.coefficients))\n",
    "        #print(\"Intercept: \" + str(temp_lrModel.intercept))\n",
    "        \n",
    "        \n",
    "       \n",
    "        rmse = evaluator.evaluate(temp_df)\n",
    "        print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 7\n",
      "Root Mean Squared Error (RMSE) on test data = 277.277\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 3\n",
      "Root Mean Squared Error (RMSE) on test data = 40.1566\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 8\n",
      "Root Mean Squared Error (RMSE) on test data = 638.485\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 0\n",
      "Root Mean Squared Error (RMSE) on test data = 562.182\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 5\n",
      "Root Mean Squared Error (RMSE) on test data = 823.759\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 6\n",
      "Root Mean Squared Error (RMSE) on test data = 197.481\n",
      "For <class 'pyspark.ml.regression.LinearRegression'> and cluster 2\n",
      "Root Mean Squared Error (RMSE) on test data = 444.264\n",
      "time: 26min 9s\n"
     ]
    }
   ],
   "source": [
    "#LinearRegression\n",
    "#masa edit w/ CV\n",
    "model_list = [LinearRegression(featuresCol=\"features\", labelCol=\"label\")]#,\\\n",
    "              #DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\"), \\\n",
    "              #GBTRegressor(featuresCol=\"features\", labelCol=\"label\"),\\\n",
    "              #RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")]\n",
    "\n",
    "\n",
    "distinct_clusters = reg_df.select('cluster').distinct().collect()\n",
    "\n",
    "cluster_numbers = [int(distinct_clusters[i][0]) for i in range(len(distinct_clusters))]\n",
    "\n",
    "for n in model_list:    \n",
    "    for i in cluster_numbers:\n",
    "        temp_df = reg_df[reg_df['cluster']==i]\n",
    "        temp_df_cv = temp_df\n",
    "        temp_lr = n\n",
    "        temp_lrModel = temp_lr.fit(temp_df['label','features'])\n",
    "        temp_df = temp_lrModel.transform(temp_df)\n",
    "        \n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(temp_lr.maxIter, [5, 10, 100]) \\\n",
    "        .addGrid(temp_lr.regParam, [0, 0.1, 0.01]) \\\n",
    "        .build()\n",
    "\n",
    "        \n",
    "        evaluator = RegressionEvaluator(\n",
    "            labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        \n",
    "        crossval = CrossValidator(estimator=temp_lr,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "        # Run cross-validation, and choose the best set of parameters.\n",
    "        cvModel = crossval.fit(temp_df_cv)\n",
    "\n",
    "        pred = cvModel.transform(temp_df_cv)\n",
    "        \n",
    "        print(\"For \" + str(type(temp_lr)) + \" and cluster \" + str(i))\n",
    "        #Print the coefficients and intercept for linear regression\n",
    "        #print(\"Coefficients: \" + str(temp_lrModel.coefficients))\n",
    "        #print(\"Intercept: \" + str(temp_lrModel.intercept))\n",
    "        \n",
    "        \n",
    "\n",
    "        rmse = evaluator.evaluate(pred)\n",
    "        print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 7\n",
      "Root Mean Squared Error (RMSE) on test data = 154.807\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 3\n",
      "Root Mean Squared Error (RMSE) on test data = 1.44338\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 8\n",
      "Root Mean Squared Error (RMSE) on test data = 0\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 0\n",
      "Root Mean Squared Error (RMSE) on test data = 263.248\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 5\n",
      "Root Mean Squared Error (RMSE) on test data = 252.745\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 6\n",
      "Root Mean Squared Error (RMSE) on test data = 0\n",
      "For <class 'pyspark.ml.regression.DecisionTreeRegressor'> and cluster 2\n",
      "Root Mean Squared Error (RMSE) on test data = 324.623\n",
      "time: 15min 32s\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeRegressor\n",
    "#masa edit w/ CV\n",
    "#[LinearRegression(featuresCol=\"features\", labelCol=\"label\"),\\\n",
    "model_list = [DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\",maxMemoryInMB=2056)]#, \\\n",
    "              #GBTRegressor(featuresCol=\"features\", labelCol=\"label\"),\\\n",
    "              #RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")]\n",
    "\n",
    "\n",
    "distinct_clusters = reg_df.select('cluster').distinct().collect()\n",
    "\n",
    "cluster_numbers = [int(distinct_clusters[i][0]) for i in range(len(distinct_clusters))]\n",
    "\n",
    "for n in model_list:    \n",
    "    for i in cluster_numbers:\n",
    "        temp_df = reg_df[reg_df['cluster']==i]\n",
    "        temp_df_cv = temp_df\n",
    "        temp_lr = n\n",
    "        temp_lrModel = temp_lr.fit(temp_df['label','features'])\n",
    "        temp_df = temp_lrModel.transform(temp_df)\n",
    "        \n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(temp_lr.maxDepth, [3, 5]) \\\n",
    "        .addGrid(temp_lr.minInfoGain, [0, 0.1, 1]) \\\n",
    "        .build()\n",
    "\n",
    "        \n",
    "        evaluator = RegressionEvaluator(\n",
    "            labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        \n",
    "        crossval = CrossValidator(estimator=temp_lr,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "        # Run cross-validation, and choose the best set of parameters.\n",
    "        cvModel = crossval.fit(temp_df_cv)\n",
    "\n",
    "        pred = cvModel.transform(temp_df_cv)\n",
    "        \n",
    "        \n",
    "        print(\"For \" + str(type(temp_lr)) + \" and cluster \" + str(i))\n",
    "        #print(cvModel.explainParams())\n",
    "\n",
    "        #Print the coefficients and intercept for linear regression\n",
    "        #print(\"Coefficients: \" + str(temp_lrModel.coefficients))\n",
    "        #print(\"Intercept: \" + str(temp_lrModel.intercept))\n",
    "        \n",
    "        \n",
    "\n",
    "        rmse = evaluator.evaluate(pred)\n",
    "        print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 7\n",
      "Root Mean Squared Error (RMSE) on test data = 28.0001\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 3\n",
      "Root Mean Squared Error (RMSE) on test data = 0.193727\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 8\n",
      "Root Mean Squared Error (RMSE) on test data = 0\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 0\n",
      "Root Mean Squared Error (RMSE) on test data = 145.209\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 5\n",
      "Root Mean Squared Error (RMSE) on test data = 3.49461\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 6\n",
      "Root Mean Squared Error (RMSE) on test data = 0\n",
      "For <class 'pyspark.ml.regression.GBTRegressor'> and cluster 2\n",
      "Root Mean Squared Error (RMSE) on test data = 223.04\n",
      "time: 33min 59s\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosted Trees\n",
    "#masa edit w/ CV\n",
    "#[LinearRegression(featuresCol=\"features\", labelCol=\"label\"),\\\n",
    "#DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\",maxMemoryInMB=1028)]#, \\             \n",
    "model_list = [GBTRegressor(featuresCol=\"features\", labelCol=\"label\", maxMemoryInMB=2056)]#,\\\n",
    "              #RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")]\n",
    "\n",
    "\n",
    "distinct_clusters = reg_df.select('cluster').distinct().collect()\n",
    "\n",
    "cluster_numbers = [int(distinct_clusters[i][0]) for i in range(len(distinct_clusters))]\n",
    "\n",
    "for n in model_list:    \n",
    "    for i in cluster_numbers:\n",
    "        temp_df = reg_df[reg_df['cluster']==i]\n",
    "        temp_df_cv = temp_df\n",
    "        temp_lr = n\n",
    "        temp_lrModel = temp_lr.fit(temp_df['label','features'])\n",
    "        temp_df = temp_lrModel.transform(temp_df)\n",
    "        \n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(temp_lr.maxDepth, [3, 5]) \\\n",
    "        .addGrid(temp_lr.maxIter, [10,20,40]) \\\n",
    "        .build()\n",
    "\n",
    "        \n",
    "        evaluator = RegressionEvaluator(\n",
    "            labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        \n",
    "        crossval = CrossValidator(estimator=temp_lr,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "        # Run cross-validation, and choose the best set of parameters.\n",
    "        cvModel = crossval.fit(temp_df_cv)\n",
    "\n",
    "        pred = cvModel.transform(temp_df_cv)\n",
    "        \n",
    "        \n",
    "        print(\"For \" + str(type(temp_lr)) + \" and cluster \" + str(i))\n",
    "        #print(cvModel.explainParams())\n",
    "\n",
    "        #Print the coefficients and intercept for linear regression\n",
    "        #print(\"Coefficients: \" + str(temp_lrModel.coefficients))\n",
    "        #print(\"Intercept: \" + str(temp_lrModel.intercept))\n",
    "        \n",
    "        \n",
    "\n",
    "        rmse = evaluator.evaluate(pred)\n",
    "        print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 7\n",
      "Root Mean Squared Error (RMSE) on test data = 144.709\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 3\n",
      "Root Mean Squared Error (RMSE) on test data = 92.0182\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 8\n",
      "Root Mean Squared Error (RMSE) on test data = 526.841\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 0\n",
      "Root Mean Squared Error (RMSE) on test data = 261.819\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 5\n",
      "Root Mean Squared Error (RMSE) on test data = 632.921\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 6\n",
      "Root Mean Squared Error (RMSE) on test data = 89.2734\n",
      "For <class 'pyspark.ml.regression.RandomForestRegressor'> and cluster 2\n",
      "Root Mean Squared Error (RMSE) on test data = 353.878\n",
      "time: 17min 48s\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Regressor\n",
    "#masa edit w/ CV\n",
    "#[LinearRegression(featuresCol=\"features\", labelCol=\"label\"),\\\n",
    "#DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\",maxMemoryInMB=1028)], \\             \n",
    "#GBTRegressor(featuresCol=\"features\", labelCol=\"label\", maxMemoryInMB=2056)],\\\n",
    "model_list = [RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", maxMemoryInMB = 2056)]\n",
    "\n",
    "\n",
    "distinct_clusters = reg_df.select('cluster').distinct().collect()\n",
    "\n",
    "cluster_numbers = [int(distinct_clusters[i][0]) for i in range(len(distinct_clusters))]\n",
    "\n",
    "for n in model_list:    \n",
    "    for i in cluster_numbers:\n",
    "        temp_df = reg_df[reg_df['cluster']==i]\n",
    "        temp_df_cv = temp_df\n",
    "        temp_lr = n\n",
    "        temp_lrModel = temp_lr.fit(temp_df['label','features'])\n",
    "        temp_df = temp_lrModel.transform(temp_df)\n",
    "        \n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(temp_lr.maxDepth, [3, 5]) \\\n",
    "        .addGrid(temp_lr.numTrees, [10,20,40]) \\\n",
    "        .build()\n",
    "\n",
    "        \n",
    "        evaluator = RegressionEvaluator(\n",
    "            labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        \n",
    "        crossval = CrossValidator(estimator=temp_lr,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "        # Run cross-validation, and choose the best set of parameters.\n",
    "        cvModel = crossval.fit(temp_df_cv)\n",
    "\n",
    "        pred = cvModel.transform(temp_df_cv)\n",
    "        \n",
    "        \n",
    "        print(\"For \" + str(type(temp_lr)) + \" and cluster \" + str(i))\n",
    "        #print(cvModel.explainParams())\n",
    "\n",
    "        #Print the coefficients and intercept for linear regression\n",
    "        #print(\"Coefficients: \" + str(temp_lrModel.coefficients))\n",
    "        #print(\"Intercept: \" + str(temp_lrModel.intercept))\n",
    "        \n",
    "        \n",
    "\n",
    "        rmse = evaluator.evaluate(pred)\n",
    "        print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
